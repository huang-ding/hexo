{"meta":{"title":"子非鱼","subtitle":"不轻信别人的结论，实践才是检验真理的唯一标准","description":"","author":"haung ding","url":"https://huang-ding.github.io/hexo","root":"/hexo/"},"pages":[{"title":"tags","date":"2020-04-26T07:10:24.000Z","updated":"2020-04-26T07:11:17.823Z","comments":false,"path":"tags/index.html","permalink":"https://huang-ding.github.io/hexo/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-04-26T07:11:28.000Z","updated":"2020-04-26T07:11:45.866Z","comments":false,"path":"categories/index.html","permalink":"https://huang-ding.github.io/hexo/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"华为鲲鹏参会摘要","slug":"kunpeng-meeting-01","date":"2020-05-29T13:03:07.000Z","updated":"2020-05-29T13:49:12.776Z","comments":true,"path":"2020/05/29/kunpeng-meeting-01/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/29/kunpeng-meeting-01/","excerpt":"","text":"鲲鹏计算产业有什么鲲鹏计算产业，根据华为官方的表述，是基于鲲鹏处理器构建的全栈 IT 基础设施、行业 应用及服务，包括 PC、服务器、存储、操作系统、中间件、虚拟化、数据库、云服务、行 业应用以及咨询管理服务等 鲲鹏与公司的联系并无直接联系，但是公司在使用的华为软开云平台和华为鲲鹏有间接联系，但是对项目本身无影响 鲲鹏与政府的联系在宁波建立了宁波鲲鹏生态产业园，并与今日拿到非民营企业执照 移植鲲鹏服务器对项目的影响 项目开发语言为java，基于jdk，鲲鹏已研发自身版本的jdk并兼容openJdk，公司现有芯爱相关使用的是openjdk可直接移植使用，政府内网使用的是oracle jdk 需要进行替换。 项目中使用到的nginx ，redis ，rabbitmq等中间件，鲲鹏已提供对应的安装包，可在鲲鹏兼容软件查询入口查询 鲲鹏提供了对应的开发套件针对于迁移的过程提供便捷和建议 华为鲲鹏分析扫描工具 华为鲲鹏代码迁移工具 华为鲲鹏性能优化工具 是否需要发展鲲鹏生态目前国外已有软件不可在中国境内使用，如HashiCorp旗下的Terraform（提供了安全有效的构建和版本控制，如kubernetes的生命周期管理，Pod的创建）Consul（可用于分布式发现注册）Vagrant； 当前公司和以上产业并无直接连接，但是按照项目的发展可能会使用到。当然国内已有一些代替产品出现如阿里的nacos即可代替Consul 目前宁波的东华软件也将借助“鹏霄”服务器，整合产业链上下游企业及自身1400余款软件，在甬加速实现落地。与此同时，东华软件将在高新区共建东华长三角总部基地、高新技术产业中试基地、数字经济应用场景实验室、数字经济产业园和院士工作站等项目，与华为在计算、工业互联网、智能家电等领域加强合作 “未来，华为与宁波有着无限的合作可能。”，鲲鹏计算产业生态涉及的合作领域众多，包括服务器与部件、虚拟化、存储、数据库、中间件、大数据平台、云服务、管理服务、行业应用9大领域，仅服务器与部件领域，市场规模就在4000亿元以上。 宁波与华为的深度携手，将为宁波企业创新发展带来新机遇，加速宁波“246”万千亿级产业集群高质量发展。为支持鲲鹏计算产业生态的建设发展，华为计划在未来五年内投资30亿元，此次华为在甬落子后，宁波将成为鲲鹏计算产业生态在长三角的关键布局城市 鲲鹏社区入口","categories":[],"tags":[{"name":"华为鲲鹏","slug":"华为鲲鹏","permalink":"https://huang-ding.github.io/hexo/tags/%E5%8D%8E%E4%B8%BA%E9%B2%B2%E9%B9%8F/"}]},{"title":"SPI机制","slug":"spi","date":"2020-05-25T11:45:04.000Z","updated":"2020-05-26T14:46:36.102Z","comments":true,"path":"2020/05/25/spi/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/25/spi/","excerpt":"","text":"SPI是什么SPI全称 Service Provider Interface ，是java提供用来被第三方实现或者扩展的API，可以用来启用框架和替换组件，让第三方提供自定义实现服务功能。 例如： JDBC驱动 加载Mysql ，Oracle ，SQL server Dubbo 扩展实现 java SPI约定 在工程的META-INF/service 目录下创建以接口名称的全路径文件，需要放在ClassPath中 在文件使用utf-8 写实现类的全路径 使用ServiceLoader.load(xxx.class)加载META-INF/service的实现类，此操作跨jar包执行 实现类需要一个无参构造方法 定义接口 12345678910111213package org.huangding.study.spi.javaspi;/** * @author huangding * @date 2020/5/25 22:20 */public interface Animal &#123; /** * @return 动物名称 */ String name();&#125; //实现接口 12345678910111213141516package org.huangding.study.spi.javaspi.impl;import org.huangding.study.spi.javaspi.Animal;/** * @author huangding * @date 2020/5/25 22:21 */public class Cat implements Animal &#123; @Override public String name() &#123; System.out.println(\"小猫猫\"); return \"小猫猫\"; &#125;&#125; 12345678910111213141516package org.huangding.study.spi.javaspi.impl;import org.huangding.study.spi.javaspi.Animal;/** * @author huangding * @date 2020/5/25 22:21 */public class Dog implements Animal &#123; @Override public String name() &#123; System.out.println(\"小狗狗\"); return \"小狗狗\"; &#125;&#125; META-INF/service 文件（org.huangding.study.spi.javaspi.Animal） 12#org.huangding.study.spi.javaspi.impl.Catorg.huangding.study.spi.javaspi.impl.Dog 使用 1234567891011121314import java.util.ServiceLoader;/** * @author huangding * @date 2020/5/25 22:26 */public class JavaSPITest &#123; public static void main(String[] args) &#123; //加载 META-INF/services文件数据 ServiceLoader&lt;Animal&gt; load = ServiceLoader.load(Animal.class); load.forEach(Animal::name); &#125;&#125; 输出 1小狗狗 Dubbo SPI机制Dubbo 并未使用 Java SPI，而是重新实现了一套功能更强的 SPI 机制。Dubbo SPI 的相关逻辑被封装在了 ExtensionLoader 类中，通过 ExtensionLoader，我们可以加载指定的实现类。Dubbo SPI 所需的配置文件需放置在 META-INF/dubbo 路径下，与 Java SPI 实现类配置不同，Dubbo SPI 是通过键值对的方式进行配置，这样我们可以按需加载指定的实现类。另外，在测试 Dubbo SPI 时，需要在提供的接口上标注 @SPI 注解. Dubbo SPI约定 在工程下创建META-INF/dubbo 使用ExtensionLoader.getExtensionLoader(xxxx.class)加载，也是跨包加载 实现和上面代码一致，接口添加 @SPI 注解 1234@SPIpublic interface Animal &#123; String name();&#125; 配置有点区别(META-INF/dubbo.com.alibaba.dubbo.demo.provider.spi.Animal) 12cat&#x3D;com.alibaba.dubbo.demo.provider.spi.Catdog&#x3D;com.alibaba.dubbo.demo.provider.spi.Dog 使用有点区别 123456789public class DubboSPITest &#123; public static void main(String[] args) &#123; ExtensionLoader&lt;Animal&gt; extensionLoader = ExtensionLoader.getExtensionLoader(Animal.class); //获取对应的实现 Animal dog = extensionLoader.getExtension(\"cat\"); System.out.println(dog.name()); &#125;&#125; doubbo spi 官方简介 SpringBoot SPI机制springboot start 使用SpringFactoriesLoader代替ServiceLoader，使用META-INF/spring.factories代替META-INF/service 1234567891011121314151617public static &lt;T&gt; List&lt;T&gt; loadFactories(Class&lt;T&gt; factoryClass, @Nullable ClassLoader classLoader) &#123; Assert.notNull(factoryClass, \"'factoryClass' must not be null\"); ClassLoader classLoaderToUse = classLoader; if (classLoaderToUse == null) &#123; classLoaderToUse = SpringFactoriesLoader.class.getClassLoader(); &#125; List&lt;String&gt; factoryNames = loadFactoryNames(factoryClass, classLoaderToUse); if (logger.isTraceEnabled()) &#123; logger.trace(\"Loaded [\" + factoryClass.getName() + \"] names: \" + factoryNames); &#125; List&lt;T&gt; result = new ArrayList&lt;&gt;(factoryNames.size()); for (String factoryName : factoryNames) &#123; result.add(instantiateFactory(factoryName, factoryClass, classLoaderToUse)); &#125; AnnotationAwareOrderComparator.sort(result); return result; &#125; spring boot读取properties文件spring.factories","categories":[],"tags":[{"name":"SPI","slug":"SPI","permalink":"https://huang-ding.github.io/hexo/tags/SPI/"}]},{"title":"springboot 杂记","slug":"spring-boot-enable-auto","date":"2020-05-17T09:13:58.000Z","updated":"2020-05-19T11:58:43.095Z","comments":true,"path":"2020/05/17/spring-boot-enable-auto/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/17/spring-boot-enable-auto/","excerpt":"","text":"@SpringBootApplication 注解这个注解相当于三个注解的功能集成 @Configuration：允许在Spring中注册额外的bean或配置 @EnableAutoConfiguration：启动Spring Boot的自动bean加载机制 @ComponentScan：在应用程序所在的包启用扫描,若未指定basePackages属性则只扫描同目录及以下的包 零Spring配置文件SpringBoot中建议放弃通过XML定义Spring 应用程序，推荐在代码类上面通过@Configuration实现配置，如果有需要还可以通过@ImportResource导入XML配置文件 个性化加载配置 节选spring boot autoconfigure 代码 使用个性化配置加载DataSource 配置 123456789@Configuration(proxyBeanMethods = false) @Conditional(PooledDataSourceCondition.class) @ConditionalOnMissingBean(&#123; DataSource.class, XADataSource.class &#125;) @Import(&#123; DataSourceConfiguration.Hikari.class, DataSourceConfiguration.Tomcat.class, DataSourceConfiguration.Dbcp2.class, DataSourceConfiguration.Generic.class, DataSourceJmxConfiguration.class &#125;) protected static class PooledDataSourceConfiguration &#123; &#125; 外部参数配置信息加载spring应用程序可以通过属性文件，yaml文件，环境变量和命令行行为参数等方式的外部化参数配置 启动时命令行传参 java -jar app.jar –name=’test’ java -jar spingboot-demo.jar –spring.profiles.active=”test” Spring Boot 配置信息中的特殊值：SPRING_APPLICATION_JSON=’{name:test}’ java -jar spingboot-demo.jar –SPRING_APPLICATION_JSON=”{&quot;spring.profiles.active&quot;:&quot;test&quot;,xxx:xxx}” 如果是web应用，可以读取ServletConfig init 参数、或ServletContext init参数 JNDI属性来自java:com/env 操作系统环境变量 配置文件application.properties,application-{profile}.properties,application.yml,application-{profile}.yml spring.profiles.active 环境变量 @PropertySource注解导入的配置：@PropertySource(value=”person.properties”) 程序入口通过SpringApplication.setDefaultProperties方法设定的参数配置 123456789101112//自定义参数@SpringBootApplicationpublic class StudyApplication &#123; public static void main(String[] args) &#123; SpringApplication springApplication = new SpringApplication(StudyApplication.class); Properties properties = new Properties(); properties.setProperty(\"name\",\"test\"); springApplication.setDefaultProperties(properties); springApplication.run(args); &#125;&#125; 环境化配置-profileprofile 是什么机制 Spring配置文件提供了一种隔离应用程序的方法，使其在特定的环境中可以使用。 可以通过profile指定Bean的应用环境（dev，test,prod） 可通过profile指定不同的环境配置参数值 指定profile 通过环境配置参数spring.profiles.active来指定应用启用的profile,默认default 在环境变量中指出，jvm参数，命令行参数，application文件中设定 代码指定：springApplication.setAdditionalProfiles(“dev”) 使用 Configuration类或者Bean方法上，添加@Profile(“dev”)注解 12345@Bean@Profile(\"dev\")public void print()&#123; System.out.println(\"测试环境\");&#125; 配置文件：xxx 配置文件配置文件可以放什么地方 当前项目运行盘符/config文件夹下面:file:./config/ 当前项目运行的目录下面（命令执行的所在目录）:file:./ classpath下面的config文件夹：classpath:/config classpath的根目录（常用）:classpath:/ 文件按照优先级排序，排在上面的会覆盖优先级低的配置 自定义配置名称和存储路径 spring.config.name=properties-file-name spring.config.location=classpath:/config/,file:./config(从右到左反序搜索) 必须将它们定义成环境属性，通常是操作系统的环境变量，JVM参数或者命令行参数 配置文件格式支持两种配置格式：.properites,.yml ymal基础语法： 大小写敏感 使用空格进行缩进（不要用tab），同级左侧对其 map键值对使用“ ：”分割 list列表使用 “ - ”表示 1234key: valuelist: - demo1 - demo2 properties实例：spring.datasource.username=test 参数使用 使用@Value(“name”)注解，将指定参数配置注入到属性 12@Value(\"$&#123;host.url:url&#125;\")provide String host; 注入Environment对象 123456@Autowiredprivate Environment environment; public String getName()&#123; return environment.getProperty(\"name\");&#125; 使用注解@ConfigurationProperties(prefix=”test”),将注解加到指定类上，spring会为实例对象的对应属性进行赋值，属性需要有getters和setters方法 123456@Component@ConfigurationProperties(prefix = \"test\")@Datapublic class TestConfig &#123; private String name;&#125; Starter快速集成作用启动器（Starter）包含许多依赖项，这些依赖项使项目快速启动和运行所需的依赖项； 例如：通过配置spring-boot-starter-data-redis，可以快捷使用spring对redis访问 命名规范官方开发的starter遵循类似命名的模式；spring-boot-starter-* 第三方starter命名规范应当遵循third party porject-spring-boot-starter 常用的starter spring-boot-starter-jdbc spring-boot-starter-data-redis spring-boot-starter-web spring-boot-starter-actuator 自研Starter的步骤 建maven工程 引入spring-boot-start ，spring-boot-autoconfiguration，第三方jar 123456789101112131415161718&lt;!--引入spring boot starter--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.2.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;version&gt;2.2.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;version&gt;2.2.1.RELEASE&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 如果需要生成配置元信息(spring-configuration-metadata.json)，加入spring-boot-configuration-processor依赖 编写配置类 12345678910111213@ConfigurationProperties(\"org.hd.my.starter\")public class MyStarterProperties &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 1234567891011@Configuration@EnableConfigurationProperties(MyStarterProperties.class)public class MyStarterAutoConfiguration &#123; @Bean public MyStarter getMyStarter(MyStarterProperties myStarterProperties) &#123; MyStarter myStarter = new MyStarter(); myStarter.setName(myStarterProperties.getName()); return myStarter; &#125;&#125; 配置发现配置文件：META-INF/spring.factories 123# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.hd.my.starter.spring.boot.autoconfiguration.MyStarterAutoConfiguration 打包发布 12345&lt;dependency&gt; &lt;groupId&gt;org.hd&lt;/groupId&gt; &lt;artifactId&gt;my-starter-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;","categories":[],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://huang-ding.github.io/hexo/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://huang-ding.github.io/hexo/tags/SpringBoot/"}]},{"title":"rpc demo 杂记","slug":"rpc-demo-coding","date":"2020-05-16T13:01:03.000Z","updated":"2020-05-16T13:05:59.612Z","comments":true,"path":"2020/05/16/rpc-demo-coding/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/16/rpc-demo-coding/","excerpt":"","text":"demo代码实现 https://github.com/huang-ding/rpc-demo 原图设计 https://www.processon.com/view/link/5ebe814ee401fd16f43c86e3","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"rpc","slug":"rpc","permalink":"https://huang-ding.github.io/hexo/tags/rpc/"}]},{"title":"RPC理论","slug":"rpc-bash","date":"2020-05-14T12:06:07.000Z","updated":"2020-05-14T13:24:10.933Z","comments":true,"path":"2020/05/14/rpc-bash/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/14/rpc-bash/","excerpt":"","text":"什么是RPCremote procedure call (RPC)：远程过程调用。 业务处理，任务计算，用本地方法一样调用远程的过程。 RPC采用Client-Server结构，通过request-response消息模式实现 RPC和RMI有什么区别 RMI: 远程方法调用(Remote Method Invocation)，它支持存储于不同地址空间的程序级对象之间彼此进行通信，实现远程对象之间的无缝远程调用。Java RMI: 用于不同虚拟机之间的通信，这些虚拟机可以在不同的主机上、也可以在同一个主机上；一个虚拟机中的对象调用另一个虚拟上中的对象的方法，只不过是允许被远程调用的对象要通过一些标志加以标识；是oop领域中RPC的一直具体实现 webservice,restfull等类似的也是RPC，仅消息的组织方式和协议不同 远程过程和本地的区别 速度相对慢，需要网络通信 可靠性减弱，比如网络不同 RPC的流程环节 客户端处理过程调用Client stub 如调用本地方法一般，传递参数； Client stub 将参数编组为消息，通过系统调用向服务端发送消息； 客户端本地操作系统将消息从客户端发送到服务器; 服务端收到数据包传递给Server stub; Server stub 解组消息为参数； Server stub 再调用服务端的过程，过程执行的结果响应给客户端 流程问题 Client stub，Server stub 的研发； 参数编组和解组； 消息如何发送； 过程结果的数据表示，异常如何处理； 如何实现安全的访问控制，防止服务攻击； PRC协议RPC调用过程中需要将参数编组为消息进行发送，接收方需要解组消息为参数，过程处理结果同样需要经过编组，解组。消息由哪些部分构成（消息头，消息体等）以及消息的表示形式(xml,json等)构成了消息协议 RPC调用过程中采用的消息协议称为RPC协议 RPC协议规定请求，响应消息的格式 在TCP上可以选用或自定义消息协议来完成RPC消息交互 我们可以选用通用的标准协议（http，https）也可以更据自身需求自定义自己的消息协议 常见的RPC协议 RPC框架封装好参数编组，消息解组，底层网络通信的RPC程序开发框架，带来的便捷是可以直接在其基础上只需专注于过程代码编写。 传统的webservice框架：Apache CXF，Apache Axis2，java自带的JAX-WS等。webservcie框架大多基于标准的SOAP协议 新兴的微服务框架：Dubbo，spring cloud，Apache Thrift 等 gRPC：一开始由 google 开发，是一款语言中立、平台中立、开源的远程过程调用(RPC)系统。 使用RPC的好处 服务化 可重用 系统交互调用 RPC相关术语 Client，Server，calls，replies，service，programs，procedures，version，marshalling（编组），unmarshalling(解组) 一个网络服务由多个远程程序集构成 一个远程程序实现一个或者多个程序过程 过程，过程的参数，过程的结果在程序协议说明中定义说明 为兼容程序协议变更过，一个服务端可能支持多个版本的远程程序","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"rpc","slug":"rpc","permalink":"https://huang-ding.github.io/hexo/tags/rpc/"}]},{"title":"分布式系统设计重要理论","slug":"distributed-core-theory","date":"2020-05-13T14:55:18.000Z","updated":"2020-05-13T14:55:24.377Z","comments":true,"path":"2020/05/13/distributed-core-theory/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/13/distributed-core-theory/","excerpt":"","text":"分布式基石-CAP定理CPA定理（CAP theorem），又称布鲁尔定理（Eric Brewer），1998年第一次提出。 最初提出是指分布式数据存储不可能同时提供以下三种保证中的两种以上 一致性（Consistency）：每次读取收到的消息是最新的。 可用性（Availability）：每个请求都会收到(非错误)响应。 分区容错性（Partition tolerance）：尽管节点之间的网络不通导致分区，系统可以继续运行 事实上，不仅仅是分布式数据存储，所有的分布式系统都必须在CAP这三点之间权衡 一般分布式系统需要保证分区容错性，在此基础上进行C和A的取舍 保证分区容错性可通过多个节点保证，但是会出现有状态服务的一致性问题，当保证节点之间状态数据一致时，可能导致系统短暂不可用，保证一致性丧失可用性，网络同步数据需要一定时间； 也有满足CAP的应用出现，实现CAP的一部分，舍弃一部分，每个地方牺牲一点，做到每个的部分满足，在业务代码做细节控制 数据一致性模型如果数据读取，写入，更新的结果是可以预测的，我们说它遵循数据一致性模型 严格一致性（Strict Consistency）（强）不论在那个节点，看到的资源都是统一的结果 顺序一致性（Sequenctial Consistency）（弱）节点的数据变动和操作的顺序保持一致，类似ZK 最终一致性（Eventual Consistency）（弱）所有数据副本最终都会变的一致，但是不保证执行顺序 强弱划分比较粗狂，但是容易理解，并发编程和分布式计算领域有更多细分模型，如会话一致性等 分布式算法基石：Paxos算法 BASE理论BASE是Basically Available（基本可用）Soft state（软状态）和Eventual consistency（最终一致性），三个短语缩写 基本可用：可能是部分功能不可用或者响应时间延长，如熔断降级处理 软状态：不同系统/节点之间，数据存在过度状态，前提是数据状态不影响业务流程，且时间有限 最终一致：经过系统内部协调机制，最终所有节点保存一致（分布式系统的一致性不一定是指数据保持一致） BASE是指导理论用于设计分布式系统，兼顾了CAP三者，降低了一致性要求，需要友好的应用程序协调机制 业务系统设计的原则墨菲定律墨菲定律（Murphy`s law）是一种心理学效应，由爱德华.墨菲提出 原句：如果有两种或两种以上的方式做某件事情，而其中一种选择方式将导致灾难，则必定有人会做出这种选择。 本质：如果事情有变坏的可能，不管可能性多小，它都会发生 如：觉得功能很简单，很快弄完，但是时间一拖再拖；觉得某个技术栈很简单，所以了解一部分之后马上就想来用，结果付出更多的时间找bug和学习；觉得某段代码复杂可能会有问题，觉得TMD居然真的有问题；觉得程序性能有问题，结果又TMD出问题。。。类似 系统设计和架构上的理解： 任何事情都没有表面看起来那么简单 所有的事情都会比你预计的时间长 会出错的事情总会出错 如果你担心每个bug出现问题，那么它就更加可能发生 康威定律设计系统的架构受制于产生这些设计的组织的沟通结构———-Conways Law 系统设计上的思考： 系统架构是公司组织架构的反映 应该按照业务闭环进行系统拆分/组织架构划分，实现闭环/高内聚/低耦合，减少沟通成本（在合适时机进行系统拆分，不要一开始就把系统服务拆分的非常细，虽然闭环，但是每个人维护的系统多，维护成本高） 如果沟通出现问题，那么应该考虑进行系统和组织架构的调整 时间再多一件事情也不可能做的完美，但总有时间做完一件事情","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"zookeeper集群","slug":"distributed-zk-cluster","date":"2020-05-12T13:27:55.000Z","updated":"2020-05-12T14:55:40.400Z","comments":true,"path":"2020/05/12/distributed-zk-cluster/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/12/distributed-zk-cluster/","excerpt":"","text":"集群特点 可靠的ZooKeeper服务 只要集群的大多数(50%)都准备好了，就可以使用服务 容错集群设置至少需要3台服务器，最好使奇数个服务器(3台和4台选举方式一致) 建议每个服务运行单独机器 配置123456789101112//心跳时间(毫秒)tickTime=2000//集群中follower服务器和Leader服务器之间完成初始化同步连接时最大心跳数。如果zk集群数量确实很大，同步时间会变长，因此可以适当调大该参数initLimit=10//集群的follower服务器与leader服务器之间请求和应答之间能够容忍最大心跳数syncLimit=5dataDir=F:/zk/colony/zk1/dataclientPort=2182//集群节点 id:通过dataDir目录创建myid的文件，一行文本只包含机器id，来为每台机器赋予一个服务id(1-255)之间；两个端口号，第一个跟随着用来连接到领导者，第二个用来选举领导者server.1=localhost:2881:3881server.2=localhost:2882:3882server.3=localhost:2883:3883 集群所有节点都可以提供服务，客户端链接时，链接串可以指定多个或全部集群节点地址，当一个节点不通时客户端自动切换 1ZkClient client = new ZkClient(\"localhost:2181,localhost:2182,localhost:2183\"); 启动日志解析12//启动的配置信息2020-05-12 20:07:49,662 [myid:2] - INFO [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@329] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 clientPortListenBacklog -1 datadir F:\\zk\\colony\\zk2\\data\\version-2 snapdir F:\\zk\\colony\\zk2\\data\\version-2 12//Ladder选举的方式2020-05-12 20:08:09,028 [myid:1] - INFO [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):FastLeaderElection@944] - New election. My id = 1, proposed zxid=0x100000013 12//当前节点的状态2020-05-12 20:08:09,101 [myid:1] - INFO [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):Follower@75] - FOLLOWING - LEADER ELECTION TOOK - 74 MS 12345//加入节点同步节点数据2020-05-12 20:08:09,114 [myid:2] - INFO [LearnerHandler-/127.0.0.1:57949:LearnerHandler@504] - Follower sid: 1 : info : localhost:2881:3881:participant2020-05-12 20:08:09,159 [myid:2] - INFO [LearnerHandler-/127.0.0.1:57949:ZKDatabase@345] - On disk txn sync enabled with snapshotSizeFactor 0.332020-05-12 20:08:09,160 [myid:2] - INFO [LearnerHandler-/127.0.0.1:57949:LearnerHandler@800] - Synchronizing with Learner sid: 1 maxCommittedLog=0x0 minCommittedLog=0x0 lastProcessedZxid=0x300000000 peerLastZxid=0x100000013 123//同步最新zxid日志[QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):Follower@170] - Got zxid 0x300000001 expected 0x1[SyncThread:1:FileTxnLog@284] - Creating new log file: log.300000001 集群监控 四字监控 JMX ZAB协议作为重要的分布式协调服务，如何保证集群数据的一致性？ 读可以所有节点读取，但是写需要先转发给leader，进行协调 所有事物请求转发给leader leader分配全局单调递增事物id(Zxid),广播事物提议 Follower处理提议，做出反馈 leader收到半数节点反馈，广播Commit Leader做出响应 返回给客户端 保证有序性，从而保证数据顺序一致性 ZAB协议-崩溃恢复Leader服务器出现奔溃，或者出现半数Follower与Leader断开联系，那么就会进入崩溃恢复模式 ZAB协议规定如果一个事物Proposal在一台机器上被处理成功(Commit)，那么应该在所有的机器上都被处理成功，哪怕机器出现奔溃故障 ZAB协议确保那些已经在Leader服务器提交的事物最终被所有服务器提交 ZAB协议确保丢弃那些只在Leader服务器提出(未ACK,未Commit)的事物 ZAB协议需要设计的选举算法应该满足： 确保提交已经被leader提交的事物Proposal，同时丢弃已经被跳过的事物Proposal 让Leader选举算法那个保证新选举的Leader服务器拥有集群最高的ZXID的事物Proposal，可以保证最高新选举的Leader一定具有所有已经提交的提按 让具有最高编号ZXID事物Proposal成为Leader可以省去Leader服务器检查Proposal的提交和丢弃工作这一步 崩溃恢复期间ZK无法提供服务 ZAB协议-数据同步Leader选举出来后，需要完成Follower与Leader的数据进行同步，当半数同步完成，可以开始提供服务 Leader服务器会为每个Follower服务器都准备一个队列，将每个Follower没有同步的事物已Proposal消息的形式逐个发送，并紧跟一个Commit消息，以表示事物已提交 Follower服务器同步事物后，Leader服务器就会将该Follower服务器加入真正的可用Follower服务器列表中，开始其他流程 ZAB协议-丢弃事物Proposal处理在ZAB协议的事物编号ZXID设计中，ZXID是一个64位数字 低32位是一个简答的单调递增计数器，Leader服务器产生一个新的Proposal的时候进行加一操作； 高32位代表Leader周期纪元编号，每次新选举Leader服务器，进行加1，同时低32位归0 基于这样的策略，当一个包含了上一个leader周期尚未提交过的事物Proposal服务器启动加入到集群时，发现此时集群已经存在leader，将自身已Follower角色链接上Leader服务器之后，Leader会更据自己服务器最后提交的Proposal和Follower服务器的Proposal进行比较，发现Follower中有上个周期的事物时，leader会要求follower进行回退操作(回退到一个确实被集群半数提交的最新事物Proposal) 选举对于选举算法的要求 选出的Leader节点上要求当前集群最高的zxid 过半节点数同意 内置实现的选举算法 LeaderElection FastLeaderElection AuthFastLeaderElection 选举机制中的概念 服务器id myid 事物id，服务器存放最大的zxid 逻辑时钟 选举状态 Looking ，竞选状态 Following 随从状态，同步Leader状态，参与投票 Observing 观察状态，同步Leader状态 不参与投票 Leading 领导者状态 选举算法 每个服务器均发起选举自己为Leader的投票(自己的给自己) 其他服务器收到投票邀请，对比事物id，比自己大的，小则不投，相等对比服务器id，大投，小不投 发起者收到大家投票反馈后，看投票数是否大于集群半数，大于则担任Leader，为超过继续发起投票 胜出条件：赞成数大于半数胜出 ZK与CAPzk保证了C(顺序一致性)和P(分区容错性)，牺牲一部分可用性(崩溃恢复无法提供服务)","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"zk","slug":"zk","permalink":"https://huang-ding.github.io/hexo/tags/zk/"}]},{"title":"zookeeper典型应用场景","slug":"distributed-zk-apply","date":"2020-05-08T11:59:13.000Z","updated":"2020-05-10T14:20:59.807Z","comments":true,"path":"2020/05/08/distributed-zk-apply/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/08/distributed-zk-apply/","excerpt":"","text":"数据发布订阅(配置中心)何为配置中心：解决系统参数配置，动态该参通知 用zookeeper实现配置中心 znode能存储数据 一个配置项一个znode 一个配置文件一个znode Watch能监听数据变化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * zk 配置中心demo */public class ConfigCenterDemo &#123; //普通配置 public static void put2Zk(String key, String value) &#123; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); if (client.exists(key)) &#123; client.writeData(key, value); &#125; else &#123; client.createPersistent(key, value); &#125; client.close(); &#125; //文件配置 public static void put2ZKFile(String key, File file) throws IOException &#123; FileInputStream fileInputStream = new FileInputStream(file); byte[] data = new byte[(int) file.length()]; fileInputStream.read(data); fileInputStream.close(); ZkClient client = new ZkClient(\"localhost:2181\"); //选择对应的序列号实现 client.setZkSerializer(new BytesPushThroughSerializer()); if (client.exists(key)) &#123; client.writeData(key, data); &#125; else &#123; client.createPersistent(key, data); &#125; client.close(); &#125; public static void get(String key) &#123; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); String value = client.readData(key); System.out.println(\"从zk读到配置\" + key + \"的值为：\" + value); //监听配置变化 client.subscribeDataChanges(key, new IZkDataListener() &#123; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; System.out.println(\"配置更新：\" + data); &#125; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; System.out.println(\"配置已删除\"); &#125; &#125;); // 这里只是为演示实时获取到配置值更新而加的等待。实际项目应用中根据具体场景写（可用阻塞方式） try &#123; Thread.sleep(5 * 60 * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; client.close(); &#125; public static void main(String[] args) throws IOException &#123;// ConfigCenterDemo.put2Zk(\"/hd\",\"666\");// ConfigCenterDemo.get(\"/hd\"); File f = new File(ConfigCenterDemo.class.getResource(\"/application.yml\").getFile()); ConfigCenterDemo.put2ZKFile(\"/hdFile\",f); ConfigCenterDemo.get(\"/hdFile\"); &#125;&#125; 命名服务何为命名服务：服务之间通过服务名称可以动态获取到服务调用地址 1234567891011121314151617181920212223242526public class NamespacesZk &#123; public static void registered(String serverName, String serverAddress) &#123; ConfigCenterDemo.put2Zk(serverName, serverAddress); &#125; public static void main(String[] args) &#123; //服务B注册 registered(\"/serverB\", \"http://www.baidu.com\"); //服务A监听 ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); String addressB = client.readData(\"/serverB\"); client.subscribeDataChanges(\"/serverB\", new IZkDataListener() &#123; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; System.out.println(\"serverB服务的地址变化\"); &#125; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; &#125; &#125;); &#125;&#125; Master选举何为Master选举：分布式服务主从结构一主多从，当主节点不可用时，自动选取出新的子节点 zookeeper实现Master选举 使用临时节点，争取创建主节点 使用临时节点防止主节点挂掉无法通知其他节点 创建servers节点，创建临时/临时顺序子节点，管理当前可用节点信息 使用临时顺序节点，可以使用最小节点当做主节点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135/** * zk Master 选举 demo */public class MasterElectionDemo &#123; static class Server &#123; //集群名称 节点名称 节点地址 private String clusterName, nodeName, nodeAddress; private String masterInfo; //当前节点目录 数据 private final String path, value; //可用节点信息 private String serversPath; private List&lt;String&gt; servers; public Server(String clusterName, String nodeName, String nodeAddress) &#123; this.clusterName = clusterName; this.nodeName = nodeName; this.nodeAddress = nodeAddress; this.path = \"/\" + this.clusterName + \"Master\"; this.serversPath = \"/\" + this.clusterName + \"servers\"; this.value = \"nodeName:\" + nodeAddress + \" nodeAddress:\" + nodeAddress; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); new Thread(new Runnable() &#123; @Override public void run() &#123; //节点注册 electionMaster(client); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; //当前存活节点数据获取 electionServers(client); &#125; &#125;).start(); &#125; private void createServers(ZkClient client) &#123; if (!client.exists(serversPath)) &#123; client.createPersistent(serversPath); &#125; try &#123; client.createEphemeral(serversPath + \"/\" + nodeName, nodeAddress); &#125; catch (ZkNodeExistsException e) &#123; client.writeData(serversPath + \"/\" + nodeName, nodeAddress); &#125; &#125; private void electionServers(ZkClient client) &#123; //阻塞等待 CountDownLatch latch = new CountDownLatch(1); IZkChildListener iZkChildListener = new IZkChildListener() &#123; @Override public void handleChildChange(String parentPath, List&lt;String&gt; currentChilds) throws Exception &#123; List&lt;String&gt; children = client.getChildren(parentPath); servers = new ArrayList&lt;&gt;(); for (String child : children) &#123; System.out.println(\"子节点消息：\" + child); servers.add(client.readData(parentPath + \"/\" + child)); &#125; System.out.println(\"存活子节点：\" + servers); latch.countDown(); &#125; &#125;; client.subscribeChildChanges(serversPath, iZkChildListener); try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; electionServers(client); &#125; private void electionMaster(ZkClient client) &#123; //创建当前存活节点信息 createServers(client); try &#123; //创建Master节点 client.createEphemeral(path, value); System.out.println(value + \"创建节点成功，成为Master\"); &#125; catch (ZkNodeExistsException e) &#123; &#125; masterInfo = client.readData(path); System.out.println(\"当前主节点为:\" + masterInfo); //阻塞等待 CountDownLatch latch = new CountDownLatch(1); IZkDataListener iZkDataListener = new IZkDataListener() &#123; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; &#125; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; //节点删除 进行主节点选举 唤醒 latch.countDown(); &#125; &#125;; //监听主节点 client.subscribeDataChanges(path, iZkDataListener); //阻塞 等待唤醒 if (client.exists(path)) &#123; try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //取消监听 client.unsubscribeDataChanges(path, iZkDataListener); //递归调用 进行下一次选举 electionMaster(client); &#125; &#125; public static void main(String[] args) &#123; // 测试时，依次开启多个Server实例java进程，然后停止获取的master的节点，看谁抢到Master// Server s = new Server(\"cluster1\", \"server1\", \"192.168.1.11:8991\"); Server s = new Server(\"cluster1\", \"server2\", \"192.168.1.11:8992\");// Server s = new Server(\"cluster1\", \"server3\", \"192.168.1.11:8993\");// Server s = new Server(\"cluster1\", \"server4\", \"192.168.1.11:8994\"); &#125;&#125; 分布式队列zookeeper实现分布式队列：使用顺序节点，入队(创建顺序节点)，出队(消费者取出所有子节点，移除最小号节点) 无界队列 有界队列 分布式锁，判断是否到达上限，防止超界 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * zk 分布式队列 简易demo * * @author huangding * @date 2020/5/10 20:18 */public class ZKQueue &#123; private final String queueName; private final int size; public ZKQueue(String queueName) &#123; this.queueName = queueName; size = Integer.MAX_VALUE; &#125; public ZKQueue(String queueName, int size) &#123; this.queueName = queueName; this.size = size; &#125; public void add(String value) &#123; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); try &#123; client.createPersistent(\"/\" + queueName); &#125; catch (ZkNodeExistsException e) &#123; &#125; List&lt;String&gt; children = client.getChildren(\"/\" + this.queueName); int childrenSize = children.size(); //TODO 正式情况需要使用分布式锁 ，有界队列 if (childrenSize &gt;= this.size) &#123; //队列已满 return; &#125; client.createPersistentSequential(\"/\" + queueName + \"/queue\", value); client.close(); System.out.println(\"入队：\" + value); &#125; public String poll() &#123; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); //TODO 正常情况消费需要加锁防止重复消费 List&lt;String&gt; children = client.getChildren(\"/\" + this.queueName); if (null == children || children.size() == 0) &#123; return null; &#125; Collections.sort(children); //移除最小节点 String s = children.get(0); String data = client.readData(\"/\" + this.queueName + \"/\" + s); client.delete(\"/\" + this.queueName + \"/\" + s); client.close(); System.out.println(\"出队：\" + data); return data; &#125; public static void main(String[] args) &#123; ZKQueue zkQueue = new ZKQueue(\"hdQueue\"); zkQueue.add(\"1\"); zkQueue.add(\"2\"); zkQueue.add(\"3\"); zkQueue.add(\"4\"); boolean f = true; do &#123; String poll = zkQueue.poll(); if (null == poll) &#123; f = false; &#125; &#125; while (f); &#125;&#125; 分布式锁zookeeper实现分布式锁： 挣抢锁，利用临时节点，原理：节点不可重名+watch;缺点：惊群效应 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143/** * zk 挣抢锁 实现demo 缺点惊群效应 */public class ZKDistributeLock implements Lock &#123; private final String lockPath; private ZkClient client; /** * 重入次数 */ private ThreadLocal&lt;Integer&gt; reentrantCount = new ThreadLocal&lt;&gt;(); public ZKDistributeLock(String lockPath) &#123; this.lockPath = \"/\" + lockPath; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); this.client = client; &#125; @Override public void lock() &#123; if (!tryLock()) &#123; //阻塞 waitForLock(); //再次尝试 lock(); &#125; &#125; private void waitForLock() &#123; CountDownLatch latch = new CountDownLatch(1); IZkDataListener listener = new IZkDataListener() &#123; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; &#125; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; //锁节点删除，开始抢锁 System.out.println(\"收到节点删除\"); latch.countDown(); &#125; &#125;; //watch监听 client.subscribeDataChanges(lockPath, listener); //阻塞 if (client.exists(lockPath)) &#123; try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 取消监听事件，进行抢锁 client.unsubscribeDataChanges(lockPath, listener); &#125; @Override public void lockInterruptibly() throws InterruptedException &#123; &#125; @Override public boolean tryLock() &#123; //可重入判断 if (reentrantCount.get() != null) &#123; Integer count = reentrantCount.get(); if (count &gt; 0) &#123; reentrantCount.set(++count); return true; &#125; &#125; try &#123; //创建锁的临时节点 client.createEphemeral(lockPath); reentrantCount.set(1); return true; &#125; catch (ZkNodeExistsException e) &#123; return false; &#125; &#125; @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; return false; &#125; @Override public void unlock() &#123; //判断重入锁释放 if (reentrantCount != null) &#123; Integer count = reentrantCount.get(); if (count &gt; 1) &#123; reentrantCount.set(--count); &#125; else &#123; reentrantCount.set(null); &#125; &#125; //删除节点，释放锁 client.delete(lockPath); &#125; @Override public Condition newCondition() &#123; return null; &#125; public static void main(String[] args) &#123; int cbCount = 50; //模拟高并发 CyclicBarrier cb = new CyclicBarrier(cbCount); for (int i = 0; i &lt; cbCount; i++) &#123; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + \"线程已准备\"); try &#123; cb.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; ZKDistributeLock lock = new ZKDistributeLock(\"testLock\"); try &#123; lock.lock(); System.out.println(Thread.currentThread().getName() + \"或得锁\"); &#125; finally &#123; lock.unlock(); &#125; &#125;).start(); &#125; &#125;&#125; 取号，利用临时顺序节点来实现分布式锁；，获取锁：取排队号（创建自己的临时顺序节点），然后判断自己是否是最小号，如是，则获得锁；不是，则注册前一节点的watcher,阻塞等待，释放锁：删除自己创建的临时顺序节点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178/** * zk 利用临时顺序节点来实现分布式锁 实现demo */public class ZKDistributeImproveLock implements Lock &#123; /* * 利用临时顺序节点来实现分布式锁 * 获取锁：取排队号（创建自己的临时顺序节点），然后判断自己是否是最小号，如是，则获得锁；不是，则注册前一节点的watcher,阻塞等待 * 释放锁：删除自己创建的临时顺序节点 */ private String lockPath; private ZkClient client; private ThreadLocal&lt;String&gt; currentPath = new ThreadLocal&lt;&gt;(); private ThreadLocal&lt;String&gt; beforePath = new ThreadLocal&lt;&gt;(); // 锁重入计数 private ThreadLocal&lt;Integer&gt; reentrantCount = new ThreadLocal&lt;&gt;(); public ZKDistributeImproveLock(String lockPath) &#123; super(); this.lockPath = lockPath; client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); if (!this.client.exists(lockPath)) &#123; try &#123; this.client.createPersistent(lockPath); &#125; catch (ZkNodeExistsException e) &#123; &#125; &#125; &#125; @Override public boolean tryLock() &#123; if (this.reentrantCount.get() != null) &#123; int count = this.reentrantCount.get(); if (count &gt; 0) &#123; this.reentrantCount.set(++count); return true; &#125; &#125; if (this.currentPath.get() == null) &#123; currentPath.set(this.client.createEphemeralSequential(lockPath + \"/\", \"aaa\")); &#125; // 获得所有的子 List&lt;String&gt; children = this.client.getChildren(lockPath); // 排序list Collections.sort(children); // 判断当前节点是否是最小的 if (currentPath.get().equals(lockPath + \"/\" + children.get(0))) &#123; this.reentrantCount.set(1); return true; &#125; else &#123; // 取到前一个 // 得到字节的索引号 int curIndex = children.indexOf(currentPath.get().substring(lockPath.length() + 1)); beforePath.set(lockPath + \"/\" + children.get(curIndex - 1)); &#125; return false; &#125; @Override public void lock() &#123; if (!tryLock()) &#123; // 阻塞等待 waitForLock(); // 再次尝试加锁 lock(); &#125; &#125; private void waitForLock() &#123; CountDownLatch cdl = new CountDownLatch(1); // 注册watcher IZkDataListener listener = new IZkDataListener() &#123; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; System.out.println(\"-----监听到节点被删除\"); cdl.countDown(); &#125; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; &#125; &#125;; client.subscribeDataChanges(this.beforePath.get(), listener); // 怎么让自己阻塞 if (this.client.exists(this.beforePath.get())) &#123; try &#123; cdl.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 醒来后，取消watcher client.unsubscribeDataChanges(this.beforePath.get(), listener); &#125; @Override public void unlock() &#123; // 重入的释放锁处理 if (this.reentrantCount.get() != null) &#123; int count = this.reentrantCount.get(); if (count &gt; 1) &#123; this.reentrantCount.set(--count); return; &#125; else &#123; this.reentrantCount.set(null); &#125; &#125; // 删除节点 this.client.delete(this.currentPath.get()); &#125; @Override public void lockInterruptibly() throws InterruptedException &#123; // TODO Auto-generated method stub &#125; @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; // TODO Auto-generated method stub return false; &#125; @Override public Condition newCondition() &#123; // TODO Auto-generated method stub return null; &#125; public static void main(String[] args) &#123; // 并发数 int currency = 50; // 循环屏障 CyclicBarrier cb = new CyclicBarrier(currency); // 多线程模拟高并发 for (int i = 0; i &lt; currency; i++) &#123; new Thread(new Runnable() &#123; public void run() &#123; System.out.println(Thread.currentThread().getName() + \"---------我准备好---------------\"); // 等待一起出发 try &#123; cb.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; ZKDistributeImproveLock lock = new ZKDistributeImproveLock(\"/distLock111\"); try &#123; lock.lock(); System.out.println(Thread.currentThread().getName() + \" 获得锁！\"); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;).start(); &#125; &#125;&#125;","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"zk","slug":"zk","permalink":"https://huang-ding.github.io/hexo/tags/zk/"}]},{"title":"zookeeper核心概念","slug":"distributed-zk-core","date":"2020-05-07T11:55:40.000Z","updated":"2020-05-08T13:51:53.100Z","comments":true,"path":"2020/05/07/distributed-zk-core/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/07/distributed-zk-core/","excerpt":"","text":"Session会话 一个客户端链接一个会话，由zk分配唯一会话id；列:session id = 0x1001bb1f6720002 客户端以特定的时间间隔发送心跳以保持会话有效；tickTime 超过会话超时时间未收到客户端心跳，则判定客户端挂了；(默认2倍tickTime) minSessionTimeout : (No Java system property) New in 3.3.0: the minimum session timeout in milliseconds that the server will allow the client to negotiate. Defaults to 2 times the tickTime. maxSessionTimeout : (No Java system property) New in 3.3.0: the maximum session timeout in milliseconds that the server will allow the client to negotiate. Defaults to 20 times the tickTime. 会话中的请求按照FIFO(fast in fast on)顺序执行. 数据模型 层次名称空间 类型Unix文件系统，以(/)为根目录 区别：节点可以包含与之关联的数据以及子节点(即是文件也是文件夹) 节点的路径总是表示未规范的，绝对的，斜杠分隔的路径 znode 名称唯一，命名规范 null字符(\\u000)不能作为路劲的一部分 \\u0001-\\u0019和\\u007F-\\u009F不能使用，因为他们不能很好的显示，或者以令人困惑的方式 \\ud800-uf8fff,\\uFFF0-uFFFF不可使用 “.”字符可以用作另一个名称的一部分，但是“.”和”..”不能单独用于指示路劲上的节点，因为ZK不使用相对路径，“/a/b/./c”或“c/b/b/../”无效 “zookeeper”是保留节点名称 节点类型 持久 1create /app1 666 顺序 10位十进制序号 每个父节点一个计算器 计数器是带符号int(4字节)，到2147483647后将溢出为负值 12create -e /app2 666nodeName: app20000000001 临时 和客户端挂钩，当客户端断开时销毁 1create -s /app3 666 临时顺序 1create -s -e /app4 666 节点数据构成 节点数据：存储的协调数据(状态信息，配置信息，位置信息等) 节点元数据(stat结构) 数据量上限：1M 访问控制列表 ACLs are made up of pairs of (scheme:expression, perms)，the pair (ip:19.22.0.0/16, READ) ACL Permissions CREATE: you can create a child node READ: you can get data from a node and list its children. WRITE: you can set data for a node DELETE: you can delete a child node Builtin ACL Schemes world has a single id, anyone, that represents anyone. auth is a special scheme which ignores any provided expression and instead uses the current user, credentials, and scheme. Any expression (whether user like with SASL authentication or user:password like with DIGEST authentication) provided is ignored by the ZooKeeper server when persisting the ACL. However, the expression must still be provided in the ACL because the ACL must match the form scheme:expression:perms. This scheme is provided as a convenience as it is a common use-case for a user to create a znode and then restrict access to that znode to only that user. If there is no authenticated user, setting an ACL with the auth scheme will fail. digest uses a username:password string to generate MD5 hash which is then used as an ACL ID identity. Authentication is done by sending the username:password in clear text. When used in the ACL the expression will be the username:base64 encoded SHA1 password digest. ip uses the client host IP as an ACL ID identity. The ACL expression is of the form addr/bits where the most significant bits of addr are matched against the most significant bits of the client host IP. x509 uses the client X500 Principal as an ACL ID identity. The ACL expression is the exact X500 Principal name of a client. When using the secure port, clients are automatically authenticated and their auth info for the x509 scheme is set. Zookeeper中的时间 Zxid zookeeper中每次更改操作都对应一个唯一的事物id，称为zxid，它是一个全局有序的戳记，如果zxid1小于zxid2，则zxid1发送在zxid2之前 Version number 版本号，对节点的每次更改都会导该改节点的版本号之一增加 *Ticks * 用多服务器Zookeeper时，服务器使用‘’滴答‘’来定义时间的类型，如状态上传，会话超时，对等点之间的链接超时。滴答时间仅提供最小会话超时(滴答2倍)间接公开；如果客户端请求的会话时间小于最小会话超时，服务器将告诉客户端实际上是最小会话超时 RealTime Zookeeper除了在znode 创建和修改时将时间戳放入stat结构之外，其他根本不使用RealTime或时钟时间 watch机制 客户端可以在znodes上设置watch，监听znode变化 设置命令 -w 代表设置watch config [-c] [-w] [-s] get [-s] [-w] path ls [-s] [-w] [-R] path stat [-w] path watch类型 getData() 监听数据变化 getChildren() 监听子节点变化 exists() 是否存在 触发watch事件 Created event: Enabled with a call to exists. Deleted event: Enabled with a call to exists, getData, and getChildren. Changed event: Enabled with a call to exists and getData. Child event: Enabled with a call to getChildren. watch的重要特性 一次性触发：watch触发后即被删除，要持续监控变化，则需要持续的设置watch 有序性：客户端先得到watch通知，后才会看到变好结果 watch注意事项 watch是一次性触发器；如果您获得一个watch事件，并且希望得到关于未来的更改通知，需要设置另一个watch 因为watch是一次性触发器，并且在获取事件和发送获取watch的新请求之前存在时间间隔，所以不能可靠的获取到节点发生的每个更改 一个watch对象只会被特定的通知触发一次。如果一个watch对象同时注册了exists和getData，当节点删除事件对exists和getData都有效，但是只会调用一次 使用zkclient 监听watch事件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class ZkClientWatchDemo &#123; public static void main(String[] args) &#123; // 创建一个zk客户端 ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); client.subscribeDataChanges(\"/mike/a\", new IZkDataListener() &#123; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; System.out.println(\"----收到节点被删除了-------------\"); &#125; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; System.out.println(\"----收到节点数据变化：\" + data + \"-------------\"); &#125; &#125;); try &#123; Thread.sleep(1000 * 60 * 2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;public class MyZkSerializer implements ZkSerializer &#123; String charset = \"UTF-8\"; @Override public Object deserialize(byte[] bytes) throws ZkMarshallingError &#123; try &#123; return new String(bytes, charset); &#125; catch (UnsupportedEncodingException e) &#123; throw new ZkMarshallingError(e); &#125; &#125; @Override public byte[] serialize(Object obj) throws ZkMarshallingError &#123; try &#123; return String.valueOf(obj).getBytes(charset); &#125; catch (UnsupportedEncodingException e) &#123; throw new ZkMarshallingError(e); &#125; &#125;&#125; Zookeeper特性 顺序一致性：保证客户端操作是按照顺序生效的 原子性：更新成功或失败，没有部分结果 单个系统映像：无论链接那个服务器，客户端都将看到相同的内容 可靠性：数据的变更不会丢失，除非被客户覆盖修改，数据变更时先写日志文件，再变更，如果是集群将发送集群变更命令，保证数据写入的可靠性 及时性：保证系统的客户端当时读取到的数据是最新的","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"zk","slug":"zk","permalink":"https://huang-ding.github.io/hexo/tags/zk/"}]},{"title":"zk入门","slug":"distributed-zk","date":"2020-05-06T11:57:23.000Z","updated":"2020-05-07T12:53:06.586Z","comments":true,"path":"2020/05/06/distributed-zk/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/06/distributed-zk/","excerpt":"","text":"什么是ZooKeeper起源：ZooKeeper 最早起源于雅虎研究院的一个研究小组。当时，雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。 简介：Apache Zookeeper是一种用于分布式应用程序的高性能协调服务，提供一种集中式信息存储服务 特点：数据存在内存中，类型文件系统的树形结构（文件和目录），高吞吐和低延迟，集群高可用 作用：基于zookeeper可以实现分布式统一配置中心，服务注册发现，分布式锁等 官网：https://zookeeper.apache.org/ 何为分布式协调服务 单机系统因处理能力上限，可用性，可靠性的考虑，变为分布式系统 原来的单机进程中完成的一件事的多个步骤，变为在多个计算机中完成，这时候就需要协调各个计算机节点做事情的顺序；原来在单系统中资源通过竞争通过锁进行同步控制；现在变成了多个计算机的进程之间的资源竞争，也需要资源协调。 我们可以把每个分布式系统中需要协调的协调管理的公共基础部分抽取出来作为一个基础公共服务，供大家使用，这就是分布式协调 zookeeper的应用案例 Hbase：使用Zookeeper进行Master选举，服务间协调 Solr：使用Zookeeper进行集群管理，Leader选举，配置中心 dubbo：服务注册发现 Mycat：集群管理，配置管理 Sharding-sphere：集群管理，配置管理 。。。。 zookeeper同类产品 consul，etcd，Doozer，等其中etcd和Doozer实现原理，存储方式和zk类似 单机版安装 官网下载对应版本压缩包 3.6.1版本安装包 解压后的conf目录，添加配置文件zoo.cfg（默认文件名，可以不使用此文件名，启动时指定便可） 启动服务端 bin/zkServer.sh start 测试，客户端链接：/bin/zkCli.sh -server 127.0.0.1:2181 123456789//单机版主要配置# The number of milliseconds of each tick 心跳时间tickTime=2000# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes. 数据存放目录dataDir=F:/zk/data# the port at which the clients will connect 客户端链接端口clientPort=2181 客户端操作命令 命令 命令参数 功能描述 addauth addauth scheme auth 创建 &amp;&amp; 验证用户 close - 关闭客户端 config config [-c] [-w] [-s] - connect connect host:port 客户端链接 create create [-s] [-e] [-c] [-t ttl] path [data] [acl] 创建节点 delete delete [-v version] path 删除节点 deleteall deleteall path 删除全部节点 rmr rmr path 递归删除节点 delquota delquota [-n |-b] path 用于删除已经创建的quota配额： get get [-s] [-w] path 获取节点值 getAcl getAcl [-s] path 获取节点信息 getAllChildrenNumber getAllChildrenNumber path 获取所有子节点数 getEphemeral getEphemerals path 获取临时节点 history - - listquota listquota path - ls ls [-s] [-w] [-R] path 获取节点信息 printwatches printwatches on off quit - 退出终端 reconfig [-s] [-v version] [[-file path] [-members serverID=host:port1:port2;port3[,…]*]] - redo redo cmdno - removewatches removewatches path [-c|-d|-a] [-1] - set set [-s] [-v version] path data 设置节点值 setAcl setAcl [-s] [-v version] [-R] path acl 设置节点权限 setquota setquota -n|-b val path 设置节点配额 stat stat [-w] path - sync sync path - version - - 第三方客户端-java12345678910111213141516171819202122&lt;!-- zkClient --&gt;&lt;!-- https://mvnrepository.com/artifact/com.101tec/zkclient --&gt;&lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;对应zk版本&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Curator --&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.curator/curator-recipes --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;对应zk版本&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt;&lt;/dependency&gt;","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"zk","slug":"zk","permalink":"https://huang-ding.github.io/hexo/tags/zk/"}]},{"title":"分布式系统架构演进之路","slug":"distributed-evolution","date":"2020-05-05T12:11:14.000Z","updated":"2020-05-05T14:45:50.583Z","comments":true,"path":"2020/05/05/distributed-evolution/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/05/distributed-evolution/","excerpt":"","text":"演进之路演进之路以一个网站为例 网站一开始就是大型的吗？ 我们应该一开始就设计一个大型网站吗？ 传统企业转型，钱多，召集上百人开发一个大型网站，等开发完结束上线，发现已经成千上百类似的网站，直接夭折。 出生 12graph LRA[无名小网站] --&gt;B[访问量低 一台服务器满足] 当业务发展越来越好，访问用户越来越多，面临问题 性能越来越差 越来越多的数据导致存储空间不足 解决方式 应用服务分离 数据服务分离 123456graph LRA[应用服务器,处理业务逻辑]B[数据库服务器,快速磁盘检索和数据缓存]C[文件服务器,存储用户上传文件]A--&gt;BA--&gt;C 不同的服务器承担不同的服务角色，并发处能利和数据存储空间性能提升 发展了一下 随着用户逐渐增加，网站再次面临挑战 数据库压力太大导致访问延迟，进行影响整个网站的性能，用户体验收到影响 当前用什么技术可以直接了当的解决问题，团队的使用了解情况，最简单的最快速的，技术的选型是有阶段性的，未到阶段不要使用太复杂的解决方案，给团队提高成本 解决方式，使用缓存改善性能 本地缓存 速度极快 数据量有限 和应用程序争抢内存 远程分布式缓存 按需扩展 性能相对本地较差 常用组件 redis memcache 系统架构图 随着用户逐渐增多，单一应用服务器面临新的问题： 能够处理的请求连接有限 网站访问高峰期 应用服务器成为网站瓶颈 解决方案 应用服务器集群，按需扩展 负载均衡调度服务器，需要高性能，高并发 软件 Apache Nginx Reverse-proxy pWEB LVS 硬件 F5 DNS负载均衡，利用域名对应多个IP 扩容（一般不考虑） 发展问题，使用缓存后虽然减轻了数据库压力，但是面临新的问题: 缓存访问不命中 缓存过期 全部的写操作都需要访问数据库 当达到一定规模后，数据库负载压力过大，成为系统瓶颈 解决方式 数据库读写分离 引发问题，数据库访问模块，数据存储层不应该和应用层代码有关系，解决方式 在Mybatis开发插件 mycat Sharding-JDBC 发展问题，用户规模导致地域越来越多，地域网络环境差别很大，面临问题 如何保证用户的访问体验 不至于访问慢流失用户 解决方案 反向代理 缓存静态资源 部署在数据中心，可以和负载均衡是同一个如nginx 地区CDN加速 适用于静态资源 部署在运营商，如电信运营商 加快用户访问速度，减轻后端服务器负载压力 发展问题，单文件服务器，单数据库，面临问题： 存不下日益增长的数据 解决问题： 分布式文件系统（存储小文件，图片） FastDFS TFS 分布式数据库系统(分库分表) Mycat Sharding-JDBC 发展问题，数据的存储需求和检索需求越来越复杂，面临问题 存储的字段差异较大，骷髅表 复杂的文本检索 解决方案： 使用NoSQL MongoDB elasticsearch 搜索引擎 lucene solr elasticsearch 发展问题，网站越来越好，业务越来越大，越来越复杂，面临问题： 应用程序变得无比庞大 迭代周期越来越快 牵一发动全身 怎么应对快速的业务发展需要 解决方案： 业务拆分，按照业务拆分多个为应用程序如，订单，首页，商家，推送 通过链接，MQ，数据存储建立连接 消息队列，RabbitMQ，ActiveMQ，Kafka 发展问题，业务规模不断增大，应用拆分越来越小，越来越多，面临问题 应用间的关系越来越复杂，应用中存在大量相同的业务操作 后端数据库成千上百台应用服务器连接，数据库连接池资源不足 解决方案 分布式服务(服务化) SOA架构 ESB 企业服务总线 中心点瓶颈 服务管理较好，可以在ESB处理 微服务 无中心点 服务框架 Dubbo SoringCloud 引用配置中心 Dubbo - zk SpringCloud config Disconf 百度 Config-toolkit 当当 Diamond 阿里 Apollo 携程 再往后，需要什么？ 数据挖掘 数据分析 推荐等业务需求 庞大的系统监控 问题分析 解决方案 大数据技术 Hadoop Spark 监控 Zabbix ElasticSearch+beats+Kibana 日志分析 ELK集中日志分析系统 以上为当前服务演进过程，当前的service mesh 还未开始琢磨，后续再修改 架构设计思想总结 分而治之 随着网站所需灵活应对 业务发展驱动技术发展，技术发展反哺业务 软件系统的价值在于它能够为用户提供什么价值，在于网站能做什么，而不在于它是怎么做的 不要新技术直接往上堆，考虑当前系统架构和团队技术栈 架构设计误区 一味追随大公司的解决方案 为了技术而技术 新技术可能存在多种系统漏洞 企图用技术解决所有问题 多探讨业务，看是否可以从业务层面解决问题 技术是用来解决业务问题的，而业务的问题，也可以通过业务的手段解决","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"设计思想","slug":"设计思想","permalink":"https://huang-ding.github.io/hexo/tags/%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/"}]},{"title":"java io 演进","slug":"evolution-io","date":"2020-04-29T13:18:53.000Z","updated":"2020-05-01T09:31:55.064Z","comments":true,"path":"2020/04/29/evolution-io/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/04/29/evolution-io/","excerpt":"","text":"JDK1.0—JDK1.3, java io类库非常原始，很多UNIX网络编程概念或接口在io库都没有体现，如：Buffer，Channel，Selector等 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//最传统单线程处理public class BIOService &#123; public static void main(String[] args) throws IOException &#123; //监听8080端口 ServerSocket socketServer = new ServerSocket(8080); System.out.println(\"启动服务端\"); while (!socketServer.isClosed()) &#123; //阻塞方法 Socket accept = socketServer.accept(); System.out.println(\"收到新链接\" + accept.toString()); try ( //接受数据 net +io InputStream inputStream = accept.getInputStream(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))) &#123; String msg; while ((msg = bufferedReader.readLine()) != null) &#123; if (msg.length() == 0) &#123; break; &#125; System.out.println(msg); &#125; System.out.println(\"收到数据：\" + accept.toString()); &#125; &#125; &#125;&#125;//客户端演示代码public class BIOClient &#123; public static void main(String[] args) &#123; try ( Socket s = new Socket(\"127.0.0.1\", 8080); OutputStream outputStream = s.getOutputStream(); Scanner scanner = new Scanner(System.in)) &#123; System.out.println(\"请输入：\"); String s1 = scanner.nextLine(); //阻塞 写完才返回 outputStream.write(s1.getBytes(StandardCharsets.UTF_8)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//使用多线程技术支持多链接 但是和线程池的大小有关 伪异步IOpublic class BIOService1 &#123; private static ExecutorService threadPool = Executors.newCachedThreadPool(); public static void main(String[] args) throws IOException &#123; //监听8080端口 ServerSocket socketServer = new ServerSocket(8080); System.out.println(\"启动服务端\"); while (!socketServer.isClosed()) &#123; //阻塞方法 Socket accept = socketServer.accept(); System.out.println(\"收到新链接\" + accept.toString()); //线程处理 threadPool.execute(() -&gt; &#123; try &#123; try ( //接受数据 net +io InputStream inputStream = accept.getInputStream(); BufferedReader bufferedReader = new BufferedReader( new InputStreamReader(inputStream, StandardCharsets.UTF_8)); OutputStream outputStream = accept.getOutputStream(); ) &#123; String msg; //读取数据阻塞 while ((msg = bufferedReader.readLine()) != null) &#123; if (msg.length() == 0) &#123; break; &#125; System.out.println(msg); &#125; System.out.println(\"收到数据：\" + accept.toString()); //返回http协议 //写入数据阻塞 outputStream.write(\"HTTP/1.1 200 ok \\r\\n\".getBytes()); outputStream.write(\"Content-Length: 11\\r\\n\\r\\n\".getBytes()); outputStream.write(\"Hello World\".getBytes()); outputStream.flush(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; &#125;&#125;//会引发的问题// 1.如果可用线程都出现故障阻塞，后续所有IO消息都将在队列排队// 2.当队列满后，后续入队操作将被阻塞// 3.新的客户端请求消息将被拒绝，客户端发送大量链接超时// 4.客户认为系统已经崩溃// NIO来解决 JDK1.4 NIO 以JSR-51 的身份正式发布，添加了java.nio包 进行异步IO的缓冲区ByteBuffer 缓存区容量(capacity) 缓冲区位置(position) 缓冲区限制(limit) 堆外缓存及堆内缓存 1234567891011//构建一个堆内内存4字节缓存区ByteBuffer byteBuffer = ByteBuffer.allocate(4);//构建一个堆外内存4字节缓存区,由于不由GC管理，使用是最好先指定JVM的最大堆外缓冲区大小限制；少一次堆拷贝ByteBuffer byteBuffer1 = ByteBuffer.allocateDirect(4);//缓冲区 相关api// flip() 读写反转// compact() 清除已读缓存区// clear() 清除整个缓冲区// rewind() 重置position为0// mark() 标记position的位置// reset() 重置position为上次mark()标记的位置 进行异步IO的管道Pipe 进行各种IO操作的Channel，包括ServerSocketChannel和SocketChannel Channel 通道，提供了NIO的非阻塞方法API，如文件FileChannel，SocketChannel，涵盖了UDP/TCP网络和文件IO，使用缓冲区进行IO操作 12345678910111213141516//SocketChannel和ServiceSocketChannel代码示例//SocketChannel//使用SocketChannel socketChannel = SocketChannel.open();//设置非阻塞模式,默认阻塞模式socketChannel.configureBlocking(false);//链接服务端socketChannel.connect(new InetSocketAddress(\"127.0.0.1\", 8080));//ServiceSocketChannel//创建网络服务端ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();//设置非阻塞serverSocketChannel.configureBlocking(false);//绑定端口serverSocketChannel.bind(new InetSocketAddress(8080)); 多种字符集的编码和解码 实现非阻塞IO操作的多路复用选择器Selector Selector选择器，可以通过Channel注册的方式进行检查管道的状态，从而实现单个线程管理多个Channel，从而提高NIO利用效率 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//Selector选择器实现public static void main(String[] args) throws Exception &#123; // 1. 创建网络服务端ServerSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); // 设置为非阻塞模式 // 2. 构建一个Selector选择器,并且将channel注册上去 Selector selector = Selector.open(); SelectionKey selectionKey = serverSocketChannel.register(selector, 0, serverSocketChannel);// 将serverSocketChannel注册到selector selectionKey.interestOps(SelectionKey.OP_ACCEPT); // 对serverSocketChannel上面的accept事件感兴趣(serverSocketChannel只能支持accept操作) // 3. 绑定端口 serverSocketChannel.socket().bind(new InetSocketAddress(8080)); System.out.println(\"启动成功\"); while (true) &#123; // 不再轮询通道,改用下面轮询事件的方式.select方法有阻塞效果,直到有事件通知才会有返回 selector.select(); // 获取事件 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); // 遍历查询结果e Iterator&lt;SelectionKey&gt; iter = selectionKeys.iterator(); while (iter.hasNext()) &#123; // 被封装的查询结果 SelectionKey key = iter.next(); iter.remove(); // 关注 Read 和 Accept两个事件 if (key.isAcceptable()) &#123; ServerSocketChannel server = (ServerSocketChannel) key.attachment(); // 将拿到的客户端连接通道,注册到selector上面 SocketChannel clientSocketChannel = server.accept(); // mainReactor 轮询accept clientSocketChannel.configureBlocking(false); clientSocketChannel.register(selector, SelectionKey.OP_READ, clientSocketChannel); System.out.println(\"收到新连接 : \" + clientSocketChannel.getRemoteAddress()); &#125; if (key.isReadable()) &#123; SocketChannel socketChannel = (SocketChannel) key.attachment(); try &#123; ByteBuffer requestBuffer = ByteBuffer.allocate(1024); while (socketChannel.isOpen() &amp;&amp; socketChannel.read(requestBuffer) != -1) &#123; // 长连接情况下,需要手动判断数据有没有读取结束 (此处做一个简单的判断: 超过0字节就认为请求结束了) if (requestBuffer.position() &gt; 0) break; &#125; if(requestBuffer.position() == 0) continue; // 如果没数据了, 则不继续后面的处理 requestBuffer.flip(); byte[] content = new byte[requestBuffer.limit()]; requestBuffer.get(content); System.out.println(new String(content)); System.out.println(\"收到数据,来自：\" + socketChannel.getRemoteAddress()); // TODO 业务操作 数据库 接口调用等等 // 响应结果 200 String response = \"HTTP/1.1 200 OK\\r\\n\" + \"Content-Length: 11\\r\\n\\r\\n\" + \"Hello World\"; ByteBuffer buffer = ByteBuffer.wrap(response.getBytes()); while (buffer.hasRemaining()) &#123; socketChannel.write(buffer); &#125; &#125; catch (IOException e) &#123; // e.printStackTrace(); key.cancel(); // 取消事件订阅 &#125; &#125; &#125; selector.selectNow(); &#125; // 问题: 此处一个selector监听所有事件,一个线程处理所有请求事件. 会成为瓶颈! 要有多线程的运用 // 使用Reactor线程模型处理问题&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226//单Reactor多线程模式public class ReactorService implements Runnable &#123; private final Selector selector; private final ServerSocketChannel serverSocketChannel; public static void main(String[] args) throws IOException &#123; ReactorService service1 = new ReactorService(8080); service1.run(); &#125; public ReactorService(int port) throws IOException &#123; //创建网络服务端 serverSocketChannel = ServerSocketChannel.open(); //设置非阻塞 serverSocketChannel.configureBlocking(false); //绑定端口 serverSocketChannel.bind(new InetSocketAddress(port)); //创建选择器 selector = Selector.open(); //注册网络服务端，监听链接创建事件 SelectionKey selectionKey = serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); //添加accept 处理事件 selectionKey.attach(new Acceptor()); System.out.println(\"启动完成\"); &#125; @Override public void run() &#123; try &#123; while (!Thread.interrupted()) &#123; //select方法有阻塞效果,直到有事件通知才会有返回 selector.select(); //获取事件 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); for (SelectionKey selectionKey : selectionKeys) &#123; //进行事件派发 dispatch(selectionKey); &#125; selectionKeys.clear(); selector.selectNow(); &#125; &#125; catch (IOException ignored) &#123; &#125; &#125; private void dispatch(SelectionKey selectionKey) &#123; //获取分发事件的附件 /Acceptor/Handler Runnable runnable = (Runnable) selectionKey.attachment(); if (null != runnable) &#123; runnable.run(); &#125; &#125; class Acceptor implements Runnable &#123; /** * 同步获取链接,保证链接获取在同一个Selector */ @Override public synchronized void run() &#123; try &#123; //获取新链接 SocketChannel socketChannel = serverSocketChannel.accept(); if (null != socketChannel) &#123; System.out.println(\"获取到新链接\"); //采用多路复用将事件分发给相应的Handler处理 new Handler(selector, socketChannel); &#125; &#125; catch (IOException i) &#123; &#125; &#125; &#125;&#125;final class Handler implements Runnable &#123; private final SocketChannel socketChannel; private final SelectionKey selectionKey; //处理业务的线程池 private static ExecutorService pool = Executors.newCachedThreadPool(); /** * 创建输入输出缓冲区 */ ByteBuffer input = ByteBuffer.allocate(1024); ByteBuffer output = ByteBuffer.allocate(1024); static final int READING = 0, SENDING = 1, PROCESSING = 3; int state = READING; public Handler(Selector selector, SocketChannel channel) throws IOException &#123; this.socketChannel = channel; //设置非阻塞 socketChannel.configureBlocking(false); //注册事件// SelectionKey.OP_ACCEPT selectionKey = socketChannel.register(selector, 0); //添加附件 selectionKey.attach(this); //表示感兴趣的事件通知 为写 selectionKey.interestOps(SelectionKey.OP_READ); //使Selector.select 返回下一次调用 selector.wakeup(); &#125; /** * 读处理 */ boolean inputIsComplete() throws IOException &#123; int read = socketChannel.read(input); if (read &gt; 0) &#123; input.flip(); byte[] content = new byte[input.limit()]; input.get(content); System.out.println(new String(content)); System.out.println(\"收到数据,来自：\" + socketChannel.getRemoteAddress()); &#125; else if (read &lt; 0) &#123; //对链路进行关闭 返回值-1 说明链路已关闭 selectionKey.cancel(); socketChannel.close(); return false; &#125; else &#123; //读取到0字节 忽略 &#125; return true; &#125; /** * 写处理 */ boolean outputIsComplete() throws IOException &#123; // 响应结果 200 String response = \"HTTP/1.1 200 OK\\r\\n\" + \"Content-Length: 11\\r\\n\\r\\n\" + \"Hello World\"; output.put(response.getBytes()); // hasRemaining 用于判断是否发送完成 防止'写半包'场景 while (output.hasRemaining()) &#123; socketChannel.write(output); &#125; output.clear(); System.out.println(\"写一点东西\"); return true; &#125; /** * 业务处理 */ void process() &#123; System.out.println(\"处理了一些业务\"); &#125; /** * 业务处理 */ void processHandOff() &#123; System.out.println(\"多线程业务处理\"); //修改当前状态 process(); state = SENDING; //表示感兴趣的事件通知 为读 selectionKey.interestOps(SelectionKey.OP_WRITE); &#125; @Override public void run() &#123; try &#123; if (state == READING) &#123; read(); &#125; else if (state == SENDING) &#123; send(); &#125; &#125; catch (IOException e) &#123; &#125; &#125; /** * 读处理 */ void read() throws IOException &#123; socketChannel.read(input); if (inputIsComplete()) &#123; //读完处理业务逻辑 线程池处理业务逻辑 state = PROCESSING; pool.execute(new Processer());// process();// //修改当前状态// state = SENDING;// //表示感兴趣的事件通知 为读// selectionKey.interestOps(SelectionKey.OP_WRITE); &#125; &#125; class Processer implements Runnable &#123; @Override public void run() &#123; processHandOff(); &#125; &#125; /** * 发送到请求方 */ void send() throws IOException &#123; if (outputIsComplete()) &#123; //写完，发送后，删除监听的事件 selectionKey.cancel(); &#125; &#125;&#125; 基于流行的Perl实现的正则表达式 文件通道FileChannel 。。。。。 新的NIO库极大的促进了java异步非阻塞编程的能力，但是还是有不完善的地方，特别是对文件的处理能力不足 没有统一的文件属性（如读写权限） API能力较弱，如目录的级联创建，递归遍历 底层存储系统的一些高级API无法使用 所有文件操作都是同步非阻塞，不支持异步文件读取 2011年JDK1.7发布。将原来的NIO类进行升级，被称为NIO2.0由JSR-203演进而来 提供可以批量获取文件属性的API，提供了标准文件系统SPI，供各个服务提供商扩展 提供AIO功能，支持基于文件的异步IO操作和针对网络套接字Socket的异步操作 完成JSR-51定义的通道功能，包括对配置和多播数据报的支持 提供异步套接字通道实现 通过juc.Future类实现异步操作的结果 在执行异步操作的时候传入CompletionHandler接口的实现类作为操作完成的回调 AIO实现demo 不需要通过多路复用选择器Selector对注册通道进行轮询操作即可实现异步读写，简化了NIO的编程模型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239//服务端public class AIOServer &#123; public static void main(String[] args) &#123; AsyncServerHandler asyncServerHandler = new AsyncServerHandler(8080); new Thread(asyncServerHandler, \"AIO-server\").start(); &#125;&#125;public class AsyncServerHandler implements Runnable &#123; private int port; CountDownLatch latch; AsynchronousServerSocketChannel asynchronousServerSocketChannel; public AsyncServerHandler(int port) &#123; try &#123; //构建异步通道 asynchronousServerSocketChannel = AsynchronousServerSocketChannel.open(); //绑定端口 asynchronousServerSocketChannel.bind(new InetSocketAddress(port)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void run() &#123; latch = new CountDownLatch(1); doAccept(); try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private void doAccept() &#123; //接受消息 asynchronousServerSocketChannel.accept(this, new AcceptCompleteHandler()); &#125;&#125;public class AcceptCompleteHandler implements CompletionHandler&lt;AsynchronousSocketChannel, AsyncServerHandler&gt; &#123; @Override public void completed(AsynchronousSocketChannel result, AsyncServerHandler attachment) &#123; //接受客户端链接，因为是异步所以继续调用accept方法，接受其他客户端链接。每当一个客户端链接成功后，再异步接受新的客户端 attachment.asynchronousServerSocketChannel.accept(attachment, this); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); //业务处理 result.read(byteBuffer, byteBuffer, new ReadCompleteHandler(result)); &#125; @Override public void failed(Throwable exc, AsyncServerHandler attachment) &#123; exc.printStackTrace(); attachment.latch.countDown(); &#125;&#125;public class ReadCompleteHandler implements CompletionHandler&lt;Integer, ByteBuffer&gt; &#123; private AsynchronousSocketChannel channel; public ReadCompleteHandler(AsynchronousSocketChannel channel) &#123; this.channel = channel; &#125; @Override public void completed(Integer result, ByteBuffer attachment) &#123; attachment.flip(); //获取数据大小 byte[] body = new byte[attachment.remaining()]; //获取数据 attachment.get(body); String req = new String(body, StandardCharsets.UTF_8); System.out.println(\"req：\" + req); //发送数据 doWrite(req); &#125; private void doWrite(String req) &#123; if (null != req &amp;&amp; req.length() &gt; 0) &#123; byte[] bytes = req.getBytes(); //创建缓冲区 ByteBuffer byteBuffer = ByteBuffer.allocate(bytes.length); byteBuffer.put(bytes); byteBuffer.flip(); //异步写入数据 channel.write(byteBuffer, byteBuffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; //未发送完成继续发送 if (attachment.hasRemaining()) &#123; channel.write(attachment, attachment, this); &#125; &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; //发生异常关闭连接 try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; //发生异常关闭连接 try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;//客户端public class AIOClient &#123; public static void main(String[] args) &#123; AsyncClientHandler asyncClientHandler = new AsyncClientHandler(\"127.0.0.1\",8080); new Thread(asyncClientHandler,\"AIOClient\").start(); &#125;&#125;public class AsyncClientHandler implements CompletionHandler&lt;Void, AsyncClientHandler&gt;, Runnable &#123; private final String host; private final int port; private AsynchronousSocketChannel channel; private CountDownLatch latch; public AsyncClientHandler(String host, int port) &#123; this.host = host; this.port = port; try &#123; channel = AsynchronousSocketChannel.open(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void run() &#123; latch = new CountDownLatch(1); //建立连接 channel.connect(new InetSocketAddress(host, port), this, this); try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; //释放连接 channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void completed(Void result, AsyncClientHandler attachment) &#123; byte[] req = \"Hello world\".getBytes(); ByteBuffer writer = ByteBuffer.allocate(req.length); writer.put(req); writer.flip(); channel.write(writer, writer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; //是否已经写完 if (attachment.hasRemaining()) &#123; //继续写 channel.write(attachment, attachment, this); &#125; else &#123; //写完获取服务端返回数据 ByteBuffer read = ByteBuffer.allocate(1024); channel.read(read, read, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; attachment.flip(); //读取数据 byte[] bytes = new byte[attachment.remaining()]; attachment.get(bytes); String body = new String(bytes, StandardCharsets.UTF_8); System.out.println(body); latch.countDown(); &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; latch.countDown(); &#125; &#125;); &#125; &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; latch.countDown(); &#125; &#125;); &#125; @Override public void failed(Throwable exc, AsyncClientHandler attachment) &#123; try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; latch.countDown(); &#125;&#125; 以上NIO演化 ，开始步入netty 开发高质量NIO程序不是一件简单的事情，除去NIO固有的复杂性和BUG，作为NIO服务端，需要处理网络的闪断，客户端重复接入，客户端安全认证，消息的编解码，半包读写等情况，如果没有足够的经验还是使用成熟NIO框架，netty是一个不错的选择","categories":[],"tags":[{"name":"nio","slug":"nio","permalink":"https://huang-ding.github.io/hexo/tags/nio/"}]},{"title":"I/O多路复用","slug":"multiplex-io","date":"2020-04-26T08:29:12.000Z","updated":"2020-04-29T13:14:24.820Z","comments":true,"path":"2020/04/26/multiplex-io/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/04/26/multiplex-io/","excerpt":"","text":"什么是I/O多路复用：通过把多个IO的阻塞复用到一个select的阻塞上，从而让系统在单线程的情况下可以处理多个客户端请求。 场景: 服务器需要同时处理多个处于监听状态或多个连接状态的套接字Socket 服务器需要同时处理多种网络协议的套接字。 目前支持I/O多路复用的系统调用有select，pselect，poll，epool，liunx在新的内核使用epoll，原因如下 支持一个进程打开的Socket描述符（FD）不受限制（仅受操作系统最大句柄数），select默认1024个，查看句柄数（cat /proc/sys/fs/file-max） I/O效率不会随着FD的条目增加而线性下降，epoll只针对‘活跃’的Socket进行操作，‘活跃’会主动调用callback函数 使用mmap加速内核与用户空间的消息传递，epoll通过内核和用户空间mmap同一块内存来实现的 epoll的API更简单","categories":[],"tags":[{"name":"nio","slug":"nio","permalink":"https://huang-ding.github.io/hexo/tags/nio/"}]},{"title":"I/O模型","slug":"nio","date":"2020-04-24T07:47:03.000Z","updated":"2020-04-29T13:14:19.930Z","comments":true,"path":"2020/04/24/nio/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/04/24/nio/","excerpt":"","text":"liunx 网络I/O模型 阻塞I/O模型：一个线程处理一个IO，进程阻塞; 非阻塞I/O模型：缓冲区无数据，返回一个EWOULDBLOCK错误，一般进行轮询检查此状态; I/O复用模型：Liunx提供select/poll,进程通过将文件描述符(fd)传递给select/poll系统调用,阻塞在select操作上，这样select/poll可以侦测多个fd是否处于就绪状态；但是fd数量有限，受到一些制约，因此liunx还提供了一个epoll的系统调用，epool使用基于驱动方式代替顺序扫码，提高性能，当有fd就绪时，立即回调函数rollback; 信号驱动I/O模型：提高开启套接字信号驱动IO功能，并通过系统调用sigaction执行信号处理函数，当数据准备就绪时，就为改进程生成SIGIO信号，通过信号回调通知应用程序调用recvform函数读取数据，并通知主循环函数处理数据 异步I/O模型：告知内核启动某个操作，并让内核在整个操作完成后通知我们。","categories":[{"name":"unix网络模型","slug":"unix网络模型","permalink":"https://huang-ding.github.io/hexo/categories/unix%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"nio","slug":"nio","permalink":"https://huang-ding.github.io/hexo/tags/nio/"},{"name":"网络模型","slug":"网络模型","permalink":"https://huang-ding.github.io/hexo/tags/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}]}],"categories":[{"name":"unix网络模型","slug":"unix网络模型","permalink":"https://huang-ding.github.io/hexo/categories/unix%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"华为鲲鹏","slug":"华为鲲鹏","permalink":"https://huang-ding.github.io/hexo/tags/%E5%8D%8E%E4%B8%BA%E9%B2%B2%E9%B9%8F/"},{"name":"SPI","slug":"SPI","permalink":"https://huang-ding.github.io/hexo/tags/SPI/"},{"name":"Spring","slug":"Spring","permalink":"https://huang-ding.github.io/hexo/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://huang-ding.github.io/hexo/tags/SpringBoot/"},{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"rpc","slug":"rpc","permalink":"https://huang-ding.github.io/hexo/tags/rpc/"},{"name":"zk","slug":"zk","permalink":"https://huang-ding.github.io/hexo/tags/zk/"},{"name":"设计思想","slug":"设计思想","permalink":"https://huang-ding.github.io/hexo/tags/%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/"},{"name":"nio","slug":"nio","permalink":"https://huang-ding.github.io/hexo/tags/nio/"},{"name":"网络模型","slug":"网络模型","permalink":"https://huang-ding.github.io/hexo/tags/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}]}