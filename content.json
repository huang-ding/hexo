{"meta":{"title":"子非鱼","subtitle":"不轻信别人的结论，实践才是检验真理的唯一标准","description":"","author":"haung ding","url":"https://huang-ding.github.io/hexo","root":"/hexo/"},"pages":[{"title":"tags","date":"2020-04-26T07:10:24.000Z","updated":"2020-04-26T07:11:17.823Z","comments":false,"path":"tags/index.html","permalink":"https://huang-ding.github.io/hexo/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-04-26T07:11:28.000Z","updated":"2020-04-26T07:11:45.866Z","comments":false,"path":"categories/index.html","permalink":"https://huang-ding.github.io/hexo/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"spring-cloud-eureka","slug":"spring-cloud-eureka","date":"2020-06-14T04:45:34.000Z","updated":"2020-06-14T07:12:15.706Z","comments":true,"path":"2020/06/14/spring-cloud-eureka/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/06/14/spring-cloud-eureka/","excerpt":"","text":"在服务注册中心的时候程序之间的集群调用，需要nginx等负载均衡中间件操作，但是当服务集群变大变多时就无法掌握了，且当服务迁移时，需要变更nginx配置，需要大量时间 问题 接口系统服务器不固定，随时可能增删机器； 接口调用方无法知晓服务具体的IP和Port地址（除非手工调整接口调用者的代码） 注册中心-Eureka介绍又称服务中心，管理各种服务功能包括服务的注册、发现、熔断、负载、降级等。 任何一个服务都不能直接去掉用，都需要通过注册中心来调用。通过服务中心来获取服务你不需要关注你调用的项目IP地址，由几台服务器组成，每次直接去服务中心获取可以使用的服务去调用既可。 由于各种服务都注册到了服务中心，就有了很多高级功能条件。比如几台服务提供相同服务来做客户端负载均衡（Ribbon）；监控服务器调用成功率来做断路器（Hystrix），移除服务列表中的故障点；监控服务调用时间来对不同的服务器设置不同的权重、智能路有（Zuul）等等。 Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务注册和发现。Eureka 采用了 C-S 的设计架构。Eureka Server 作为服务注册功能的服务器，它是服务注册中心。而系统中的其他微服务，使用 Eureka 的客户端连接到 Eureka Server，并维持心跳连接。这样系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行。Spring Cloud 的一些其他模块（比如Zuul）就可以通过 Eureka Server 来发现系统中的其他微服务，并执行相关的逻辑。 Eureka由两个组件组成：Eureka服务器和Eureka客户端。Eureka服务器用作服务注册服务器。Eureka客户端是一个java客户端，用来简化与服务器的交互、作为轮询负载均衡器，并提供服务的故障切换支持。Netflix在其生产环境中使用的是另外的客户端，它提供基于流量、资源利用率以及出错状态的加权负载均衡。 Eureka的作用服务提供者在启动时，定时push EurekaServer注册自己的服务信息，如服务名，IP，端口，等信息 服务消费者在启动时，定时拉取EurekaServer中存储的服务节点信息 集成Eureka依赖 12345 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;version&gt;springboot支持的版本&lt;/version&gt;&lt;/dependency&gt; 配置 12345678910111213141516171819202122232425262728server: port: 10001spring: application: name: eureka-demo# eureka 配置eureka: instance: #配置eureka实例 hostname: dev instance-id: dev client: #eureka客户端配置 fetch-registry: false #是否去注册中心需要拉取信息 register-with-eureka: false #是否需要注册到eureka #地址分区# region: beijing# availability-zones:# beijing: zone-1,zone-2 #分区节点 service-url: # 分区服务地址 defaultZone: http://localhost:$&#123;server.port&#125;/eureka #默认eureka注册地址# zone-1: http://localhost:9002/eureka #分区服务节点地址# zone-2: http://localhost:9003/eureka #分区服务节点地址 server: #eureka服务端配置 wait-time-in-ms-when-sync-empty: 0 #同步等待时间 enable-self-preservation: true #启动自我保护机制 peer-eureka-nodes-update-interval-ms: 100000 #更新节点执行间隔 在@SpringBootApplication添加启动注解 1@EnableEurekaServer image-20200607204823926 使用依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;springboot支持的版本&lt;/version&gt;&lt;/dependency&gt; application.yml配置 1234eureka: client: service-url: defaultZone: http://localhost:10001/eureka #默认eureka注册地址 在@SpringBootApplication添加启动注解 1@EnableEurekaClient 服务者提供rest接口 1234@GetMapping(\"\")public Object index() &#123; return \"这是服务端:\" + applicationName + \":\" + port + \"返回的应答\";&#125; 消费者使用RestTemplate调用接口 123456789101112@Bean@LoadBalanced //用于负载均衡 默认轮询public RestTemplate restTemplate()&#123; return new RestTemplate();&#125;@Autowiredprivate RestTemplate restTemplate;@GetMapping(\"index\")public Object getIndex()&#123; return restTemplate.getForObject(\"http://hello-server-demo/\",String.class,\"\");&#125; 启动时服务是如何注册到Eureka的 image-20200608210751108 服务提供者在启动时，向Eureka发送注册的http（POST /eureka/v2/apps/appID）请求 Eureka Client 代码节选 12345678910111213141516171819202122232425// default size of 2 - 1 each for heartbeat and cacheRefresh //定时执行线程 scheduler = Executors.newScheduledThreadPool(2, new ThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient-%d\") .setDaemon(true) .build()); // 心跳线程 heartbeatExecutor = new ThreadPoolExecutor( 1, clientConfig.getHeartbeatExecutorThreadPoolSize(), 0, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), new ThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient-HeartbeatExecutor-%d\") .setDaemon(true) .build() ); // use direct handoff //缓存刷新线程 cacheRefreshExecutor = new ThreadPoolExecutor( 1, clientConfig.getCacheRefreshExecutorThreadPoolSize(), 0, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), new ThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient-CacheRefreshExecutor-%d\") .setDaemon(true) .build() ); // use direct handoff 1234567891011121314151617181920212223@Override public EurekaHttpResponse&lt;Void&gt; register(InstanceInfo info) &#123; String urlPath = \"apps/\" + info.getAppName(); ClientResponse response = null; try &#123; Builder resourceBuilder = jerseyClient.resource(serviceUrl).path(urlPath).getRequestBuilder(); addExtraHeaders(resourceBuilder); response = resourceBuilder .header(\"Accept-Encoding\", \"gzip\") .type(MediaType.APPLICATION_JSON_TYPE) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class, info); return anEurekaHttpResponse(response.getStatus()).headers(headersOf(response)).build(); &#125; finally &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"Jersey HTTP POST &#123;&#125;/&#123;&#125; with instance &#123;&#125;; statusCode=&#123;&#125;\", serviceUrl, urlPath, info.getId(), response == null ? \"N/A\" : response.getStatus()); &#125; if (response != null) &#123; response.close(); &#125; &#125; &#125; 服务端如何保存这些信息，Eureka都请求的数据进行校验保存更新到Eureka的一个Map集合放入到内存中，key为服务名称，应可能出现同名服务，所以value值应该类似一个list集合 保存的Map 12private final ConcurrentHashMap&lt;String, Map&lt;String, Lease&lt;InstanceInfo&gt;&gt;&gt; registry = new ConcurrentHashMap&lt;String, Map&lt;String, Lease&lt;InstanceInfo&gt;&gt;&gt;(); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182//com.netflix.eureka.registry.AbstractInstanceRegistrypublic void register(InstanceInfo registrant, int leaseDuration, boolean isReplication) &#123; try &#123; read.lock(); Map&lt;String, Lease&lt;InstanceInfo&gt;&gt; gMap = registry.get(registrant.getAppName()); REGISTER.increment(isReplication); if (gMap == null) &#123; final ConcurrentHashMap&lt;String, Lease&lt;InstanceInfo&gt;&gt; gNewMap = new ConcurrentHashMap&lt;String, Lease&lt;InstanceInfo&gt;&gt;(); gMap = registry.putIfAbsent(registrant.getAppName(), gNewMap); if (gMap == null) &#123; gMap = gNewMap; &#125; &#125; Lease&lt;InstanceInfo&gt; existingLease = gMap.get(registrant.getId()); // Retain the last dirty timestamp without overwriting it, if there is already a lease if (existingLease != null &amp;&amp; (existingLease.getHolder() != null)) &#123; Long existingLastDirtyTimestamp = existingLease.getHolder().getLastDirtyTimestamp(); Long registrationLastDirtyTimestamp = registrant.getLastDirtyTimestamp(); logger.debug(\"Existing lease found (existing=&#123;&#125;, provided=&#123;&#125;\", existingLastDirtyTimestamp, registrationLastDirtyTimestamp); // this is a &gt; instead of a &gt;= because if the timestamps are equal, we still take the remote transmitted // InstanceInfo instead of the server local copy. if (existingLastDirtyTimestamp &gt; registrationLastDirtyTimestamp) &#123; logger.warn(\"There is an existing lease and the existing lease's dirty timestamp &#123;&#125; is greater\" + \" than the one that is being registered &#123;&#125;\", existingLastDirtyTimestamp, registrationLastDirtyTimestamp); logger.warn(\"Using the existing instanceInfo instead of the new instanceInfo as the registrant\"); registrant = existingLease.getHolder(); &#125; &#125; else &#123; // The lease does not exist and hence it is a new registration synchronized (lock) &#123; if (this.expectedNumberOfClientsSendingRenews &gt; 0) &#123; // Since the client wants to register it, increase the number of clients sending renews this.expectedNumberOfClientsSendingRenews = this.expectedNumberOfClientsSendingRenews + 1; updateRenewsPerMinThreshold(); &#125; &#125; logger.debug(\"No previous lease information found; it is new registration\"); &#125; Lease&lt;InstanceInfo&gt; lease = new Lease&lt;InstanceInfo&gt;(registrant, leaseDuration); if (existingLease != null) &#123; lease.setServiceUpTimestamp(existingLease.getServiceUpTimestamp()); &#125; gMap.put(registrant.getId(), lease); synchronized (recentRegisteredQueue) &#123; recentRegisteredQueue.add(new Pair&lt;Long, String&gt;( System.currentTimeMillis(), registrant.getAppName() + \"(\" + registrant.getId() + \")\")); &#125; // This is where the initial state transfer of overridden status happens if (!InstanceStatus.UNKNOWN.equals(registrant.getOverriddenStatus())) &#123; logger.debug(\"Found overridden status &#123;&#125; for instance &#123;&#125;. Checking to see if needs to be add to the \" + \"overrides\", registrant.getOverriddenStatus(), registrant.getId()); if (!overriddenInstanceStatusMap.containsKey(registrant.getId())) &#123; logger.info(\"Not found overridden id &#123;&#125; and hence adding it\", registrant.getId()); overriddenInstanceStatusMap.put(registrant.getId(), registrant.getOverriddenStatus()); &#125; &#125; InstanceStatus overriddenStatusFromMap = overriddenInstanceStatusMap.get(registrant.getId()); if (overriddenStatusFromMap != null) &#123; logger.info(\"Storing overridden status &#123;&#125; from map\", overriddenStatusFromMap); registrant.setOverriddenStatus(overriddenStatusFromMap); &#125; // Set the status based on the overridden status rules InstanceStatus overriddenInstanceStatus = getOverriddenInstanceStatus(registrant, existingLease, isReplication); registrant.setStatusWithoutDirty(overriddenInstanceStatus); // If the lease is registered with UP status, set lease service up timestamp if (InstanceStatus.UP.equals(registrant.getStatus())) &#123; lease.serviceUp(); &#125; registrant.setActionType(ActionType.ADDED); recentlyChangedQueue.add(new RecentlyChangedItem(lease)); registrant.setLastUpdatedTimestamp(); invalidateCache(registrant.getAppName(), registrant.getVIPAddress(), registrant.getSecureVipAddress()); logger.info(\"Registered instance &#123;&#125;/&#123;&#125; with status &#123;&#125; (replication=&#123;&#125;)\", registrant.getAppName(), registrant.getId(), registrant.getStatus(), isReplication); &#125; finally &#123; read.unlock(); &#125; &#125; 消费者如何根据服务名称发现服务实例服务消费者在启动时，向Eureka发送http（GET /eureka/v2/apps/appID）请求，已服务名称为参数，拉取对应Map服务的数据，如服务提供者名称，ip，端口，等信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061//com.netflix.discovery.DiscoveryClientclass CacheRefreshThread implements Runnable &#123; public void run() &#123; refreshRegistry(); &#125; &#125; @VisibleForTesting void refreshRegistry() &#123; try &#123; boolean isFetchingRemoteRegionRegistries = isFetchingRemoteRegionRegistries(); boolean remoteRegionsModified = false; // This makes sure that a dynamic change to remote regions to fetch is honored. String latestRemoteRegions = clientConfig.fetchRegistryForRemoteRegions(); if (null != latestRemoteRegions) &#123; String currentRemoteRegions = remoteRegionsToFetch.get(); if (!latestRemoteRegions.equals(currentRemoteRegions)) &#123; // Both remoteRegionsToFetch and AzToRegionMapper.regionsToFetch need to be in sync synchronized (instanceRegionChecker.getAzToRegionMapper()) &#123; if (remoteRegionsToFetch.compareAndSet(currentRemoteRegions, latestRemoteRegions)) &#123; String[] remoteRegions = latestRemoteRegions.split(\",\"); remoteRegionsRef.set(remoteRegions); instanceRegionChecker.getAzToRegionMapper().setRegionsToFetch(remoteRegions); remoteRegionsModified = true; &#125; else &#123; logger.info(\"Remote regions to fetch modified concurrently,\" + \" ignoring change from &#123;&#125; to &#123;&#125;\", currentRemoteRegions, latestRemoteRegions); &#125; &#125; &#125; else &#123; // Just refresh mapping to reflect any DNS/Property change instanceRegionChecker.getAzToRegionMapper().refreshMapping(); &#125; &#125; boolean success = fetchRegistry(remoteRegionsModified); if (success) &#123; registrySize = localRegionApps.get().size(); lastSuccessfulRegistryFetchTimestamp = System.currentTimeMillis(); &#125; if (logger.isDebugEnabled()) &#123; StringBuilder allAppsHashCodes = new StringBuilder(); allAppsHashCodes.append(\"Local region apps hashcode: \"); allAppsHashCodes.append(localRegionApps.get().getAppsHashCode()); allAppsHashCodes.append(\", is fetching remote regions? \"); allAppsHashCodes.append(isFetchingRemoteRegionRegistries); for (Map.Entry&lt;String, Applications&gt; entry : remoteRegionVsApps.entrySet()) &#123; allAppsHashCodes.append(\", Remote region: \"); allAppsHashCodes.append(entry.getKey()); allAppsHashCodes.append(\" , apps hashcode: \"); allAppsHashCodes.append(entry.getValue().getAppsHashCode()); &#125; logger.debug(\"Completed cache refresh task for discovery. All Apps hash code is &#123;&#125; \", allAppsHashCodes); &#125; &#125; catch (Throwable e) &#123; logger.error(\"Cannot fetch registry from server\", e); &#125; &#125; 如何构建高可用的Eureka集群Eureka使用了对等协议，进行类似组播广播一样的数据同步 在eureka高可用的状态下，这些注册中心是对等的，他们会互相将注册在自己的实例同步给其他的注册中心 1234567891011121314151617181920212223242526272829303132333435363738394041424344//com.netflix.eureka.cluster.PeerEurekaNodesprotected void updatePeerEurekaNodes(List&lt;String&gt; newPeerUrls) &#123; if (newPeerUrls.isEmpty()) &#123; logger.warn(\"The replica size seems to be empty. Check the route 53 DNS Registry\"); return; &#125; Set&lt;String&gt; toShutdown = new HashSet&lt;&gt;(peerEurekaNodeUrls); toShutdown.removeAll(newPeerUrls); Set&lt;String&gt; toAdd = new HashSet&lt;&gt;(newPeerUrls); toAdd.removeAll(peerEurekaNodeUrls); if (toShutdown.isEmpty() &amp;&amp; toAdd.isEmpty()) &#123; // No change return; &#125; // Remove peers no long available List&lt;PeerEurekaNode&gt; newNodeList = new ArrayList&lt;&gt;(peerEurekaNodes); if (!toShutdown.isEmpty()) &#123; logger.info(\"Removing no longer available peer nodes &#123;&#125;\", toShutdown); int i = 0; while (i &lt; newNodeList.size()) &#123; PeerEurekaNode eurekaNode = newNodeList.get(i); if (toShutdown.contains(eurekaNode.getServiceUrl())) &#123; newNodeList.remove(i); eurekaNode.shutDown(); &#125; else &#123; i++; &#125; &#125; &#125; // Add new peers if (!toAdd.isEmpty()) &#123; logger.info(\"Adding new peer nodes &#123;&#125;\", toAdd); for (String peerUrl : toAdd) &#123; newNodeList.add(createPeerEurekaNode(peerUrl)); &#125; &#125; this.peerEurekaNodes = newNodeList; this.peerEurekaNodeUrls = new HashSet&lt;&gt;(newPeerUrls); &#125; 心跳和服务剔除机制是什么在Eureka启动时创建定时线程，进行定时轮询，判断是否达到剔除的临近阀值，如果启动了自我保护机制的Eureka实例则忽略 12345678//com.netflix.eureka.registry.AbstractInstanceRegistry//临界值计算 每分钟收到的心跳数protected void updateRenewsPerMinThreshold() &#123; //发送过的服务次数*(60/心跳间隔)*自定义的阈值 this.numberOfRenewsPerMinThreshold = (int) (this.expectedNumberOfClientsSendingRenews * (60.0 / serverConfig.getExpectedClientRenewalIntervalSeconds()) * serverConfig.getRenewalPercentThreshold()); &#125; 心跳日志 1234567891011121314152020-06-08 20:32:36.103 DEBUG 7772 --- [t-Conn-Cleaner2] c.n.d.shared.MonitoredConnectionManager : Closing connections idle longer than 30 SECONDS2020-06-08 20:32:36.103 DEBUG 7772 --- [t-Conn-Cleaner2] c.n.d.shared.NamedConnectionPool : Closing connections idle longer than 30 SECONDS2020-06-08 20:32:49.810 DEBUG 7772 --- [io-10001-exec-5] o.s.c.n.eureka.server.InstanceRegistry : renew HELLO-SERVER-DEMO serverId localhost:hello-server-demo:20001, isReplication &#123;&#125;false2020-06-08 20:32:49.810 DEBUG 7772 --- [io-10001-exec-5] c.n.e.registry.AbstractInstanceRegistry : Fetching applications registry with remote regions: false, Regions argument []2020-06-08 20:32:49.811 DEBUG 7772 --- [io-10001-exec-5] c.n.e.registry.AbstractInstanceRegistry : Processing override status using rule: [com.netflix.eureka.registry.rule.DownOrStartingRule, com.netflix.eureka.registry.rule.OverrideExistsRule, com.netflix.eureka.registry.rule.LeaseExistsRule, com.netflix.eureka.registry.rule.AlwaysMatchInstanceStatusRule]2020-06-08 20:32:49.812 DEBUG 7772 --- [io-10001-exec-5] c.n.e.registry.rule.LeaseExistsRule : There is already an existing lease with status UP for instance localhost:hello-server-demo:200012020-06-08 20:32:49.813 DEBUG 7772 --- [io-10001-exec-5] c.n.eureka.resources.InstanceResource : Found (Renew): HELLO-SERVER-DEMO - localhost:hello-server-demo:20001; reply status&#x3D;2002020-06-08 20:32:50.313 DEBUG 7772 --- [et_localhost-15] c.n.d.shared.MonitoredConnectionManager : Get connection: &#123;&#125;-&gt;http:&#x2F;&#x2F;localhost:10001, timeout &#x3D; 2002020-06-08 20:32:50.314 DEBUG 7772 --- [et_localhost-15] c.n.d.shared.NamedConnectionPool : [&#123;&#125;-&gt;http:&#x2F;&#x2F;localhost:10001] total kept alive: 1, total issued: 0, total allocated: 1 out of 10002020-06-08 20:32:50.314 DEBUG 7772 --- [et_localhost-15] c.n.d.shared.NamedConnectionPool : Getting free connection [&#123;&#125;-&gt;http:&#x2F;&#x2F;localhost:10001][null]2020-06-08 20:32:50.319 DEBUG 7772 --- [io-10001-exec-6] c.n.d.util.DeserializerStringCache : clearing global-level cache with size 02020-06-08 20:32:50.319 DEBUG 7772 --- [io-10001-exec-6] c.n.d.util.DeserializerStringCache : clearing app-level serialization cache with size 02020-06-08 20:32:50.319 DEBUG 7772 --- [io-10001-exec-6] o.s.c.n.eureka.server.InstanceRegistry : renew HELLO-SERVER-DEMO serverId localhost:hello-server-demo:20001, isReplication &#123;&#125;true2020-06-08 20:32:50.319 DEBUG 7772 --- [io-10001-exec-6] c.n.e.registry.AbstractInstanceRegistry : Fetching applications registry with remote regions: false, Regions argument []2020-06-08 20:32:50.320 DEBUG 7772 --- [io-10001-exec-6] c.n.e.registry.AbstractInstanceRegistry : Processing override status using rule: [com.netflix.eureka.registry.rule.DownOrStartingRule, com.netflix.eureka.registry.rule.OverrideExistsRule, com.netflix.eureka.registry.rule.LeaseExistsRule, com.netflix.eureka.registry.rule.AlwaysMatchInstanceStatusRule] Eureka自我保护模式是什么短时间不可用或分区导致心跳时间延时过长，不会直接剔除，继续存在集群中，保证可用性(AP)，但是存在调用不可用Eureka，此时可配合熔断降级一起使用 123456789//自我保护机制判断@Override public boolean isLeaseExpirationEnabled() &#123; if (!isSelfPreservationModeEnabled()) &#123; // The self preservation mode is disabled, hence allowing the instances to expire. return true; &#125; return numberOfRenewsPerMinThreshold &gt; 0 &amp;&amp; getNumOfRenewsInLastMin() &gt; numberOfRenewsPerMinThreshold; &#125; 1eureka.server.enable-self-preservation: true #启动自我保护机制 Eureka REST 调用流程图","categories":[],"tags":[{"name":"spring-cloud","slug":"spring-cloud","permalink":"https://huang-ding.github.io/hexo/tags/spring-cloud/"}]},{"title":"docker-tool","slug":"docker-tool","date":"2020-06-09T07:25:34.000Z","updated":"2020-06-09T07:34:54.453Z","comments":true,"path":"2020/06/09/docker-tool/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/06/09/docker-tool/","excerpt":"","text":"一些常用的docker 命令删除容器日志 1docker inspect 【容器名称】 | grep LogPath | cut -d ':' -f 2 | cut -d ',' -f 1 | xargs echo | xargs truncate -s 0 限制容器日志大小 12345678910vim &#x2F;etc&#x2F;docker&#x2F;daemon.json&#123; &quot;log-driver&quot;:&quot;json-file&quot;, &quot;log-opts&quot;: &#123;&quot;max-size&quot;:&quot;50m&quot;, &quot;max-file&quot;:&quot;3&quot;&#125;&#125;#重启dockersystemctl daemon-reloadsystemctl restart docker max-size=50m，意味着一个容器日志大小上限是50M，max-file=3，意味着一个容器有三个日志，分别是id+.json、id+1.json、id+2.json。","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://huang-ding.github.io/hexo/tags/Docker/"}]},{"title":"Docker 原地址变更","slug":"docker-address-change","date":"2020-06-09T06:54:14.000Z","updated":"2020-06-09T06:55:32.472Z","comments":true,"path":"2020/06/09/docker-address-change/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/06/09/docker-address-change/","excerpt":"","text":"停止docker 1service docker stop 创建移动的文件目录 1mkdir /mnt/sdc/docker 复制docker源文件到新目录 1mv /var/lib/docker/* /mnt/sdc/docker/ 创建软连接 1ln -s /mnt/sdc/docker/ /var/lib/docker 重新启动docker 1service docker start 查看docker 信息默认地址是否发生变更 12docker info | grep -i Docker | grep Root# Docker Root Dir: /mnt/sdc/docker","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://huang-ding.github.io/hexo/tags/Docker/"}]},{"title":"dubbo一部分特性","slug":"dubbo-characteristic","date":"2020-06-03T12:16:32.000Z","updated":"2020-06-04T12:43:40.608Z","comments":true,"path":"2020/06/03/dubbo-characteristic/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/06/03/dubbo-characteristic/","excerpt":"","text":"启动时检查Dubbo缺省会在启动时检查服务是否可用，不可用时抛出异常，阻止程序(spring)初始化完成 通过check=&quot;true/false&quot;配置,可通过多种方式，spring配置文件，dubbo配置文件，jvm -D参数等 spring配置示例文件123&lt;dubbo:reference interface=\"com.foo.BarService\" check=\"false\" /&gt;&lt;dubbo:consumer check=\"false\" /&gt;&lt;dubbo:registry check=\"false\" /&gt; dubbo.properties1234dubbo.reference.com.foo.BarService.check=falsedubbo.reference.check=falsedubbo.consumer.check=falsedubbo.registry.check=false 多版本当接口升级出现不兼容时，可以通过版本号进行过度，version=1.0.0,版本号不同的服务互相不引用 示例老版本 1&lt;dubbo:reference id=\"barService\" interface=\"com.foo.BarService\" version=\"1.0.0\" /&gt; 新版本 1&lt;dubbo:reference id=\"barService\" interface=\"com.foo.BarService\" version=\"2.0.0\" /&gt; 不区分版本 1&lt;dubbo:reference id=\"barService\" interface=\"com.foo.BarService\" version=\"*\" /&gt; 服务分组当一个接口有多种实现时，可用group区分 http group分组是否bug 2.7 ？ 示例 提供者 1&lt;dubbo:service group=\"feedback\" interface=\"com.xxx.IndexService\" /&gt; &lt;dubbo:service group=\"member\" interface=\"com.xxx.IndexService\" /&gt; 消费者 1&lt;dubbo:reference id=\"feedbackIndexService\" group=\"feedback\" interface=\"com.xxx.IndexService\" /&gt; &lt;dubbo:reference id=\"memberIndexService\" group=\"member\" interface=\"com.xxx.IndexService\" /&gt; 任意组 1&lt;dubbo:reference id=\"barService\" interface=\"com.foo.BarService\" group=\"*\" /&gt; dubbo提供分组聚合，分组聚合文档：分组聚合 合并结果扩展：结果扩展 参数验证参数验证基于 JSR303 实现,用户只需使用JSR303标准的注解，并通过filter来实现 示例 @NotNull,@Siez,@Min 等 参数验证文档：参数验证 本地伪装本地伪装通常用于服务降级，当服务提供者挂了，客户端不会抛出异常，通过Mock数据返回 示例 12&lt;dubbo:reference interface=\"com.foo.BarService\" mock=\"true\" /&gt;&lt;dubbo:reference interface=\"com.foo.BarService\" mock=\"com.foo.BarServiceMock\" /&gt; 客户端提供Mock实现类，需和提供者实现同一个接口 12345public class BarServiceMock implements BarService &#123; public String sayHello(String name) &#123; return \"容错数据\"; &#125;&#125; return可以使用return 返回一个字符串表示的对象 empty：空，基本类型默认值，集合的空值 null：null true/false：true/false JSON：反序列化JSON所得到的对象 throw使用 throw来返回一个异常对象，作为mock的返回值 forec 和 failforec 强制使用mock行为，不走远程调用，fail与默认行为一直 本地存根使用stub，由stub验证决定是否需要调用远程服务Proxy实例 action-&gt;stub-&gt;Proxy-&gt;impl 示例 1&lt;dubbo:service interface=\"com.foo.BarService\" stub=\"true\" /&gt;&lt;dubbo:service interface=\"com.foo.BarService\" stub=\"com.foo.BarServiceStub\" /&gt; stub实现 1234567891011121314151617public class BarServiceStub implements BarService &#123; private final BarService barService; // 构造函数传入真正的远程代理对象 public BarServiceStub(BarService barService)&#123; this.barService = barService; &#125; public String sayHello(String name) &#123; // 此代码在客户端执行, 你可以在客户端做ThreadLocal本地缓存，或预先验证参数是 否合法，等等 try &#123; return barService.sayHello(name); &#125; catch (Exception e) &#123; // 你可以容错，可以做任何AOP拦截事项 return \"容错数据\"; &#125; &#125;&#125; Stub必须提供一个Proxy的构造函数，且需要实现Proxy的接口 负载均衡在集群负载均衡时，dubbo提供了多种均衡策略，缺省为random随机 负载策略 LoadBalance Random 随机 RoundRobin 轮询 LeastActive 最少活跃 ConsistentHash 一致性hash 负载均衡文档：负载 集群容错在集群调度失败时，dubbo提供了多种容错方案，缺省为failover重试 模式 Failover 重试其他服务器 Failfast 快速失败，调用一次 Failsafe 安全失败 Failback 失败自动恢复，定时重发 Forking 并行调用，一个成功即可 Broadcast 广播所有，需要全部成功 集群文档：集群 线程模型ThreadPool fixed 固定大小线程池 cached 缓存线程池 文档地址：文档 并发控制限制服务方法，服务提供者并发执行的限制 对方法单独设置现在 对每个客户端进行并发控制 使用load balance 调用最小活跃的服务提供者 在服务卡顿是，并发量太大可以调整此参数 文档地址：文档 Provider异步执行和Conumer异步调用文档地址：执行,调用 链接控制设置服务提供者连接数 限制客户端服务使用链接数 如果是长链接，代表长链接的上线 日志适配支持 log4j,slf4j、jcl、jdk 这些日志框架的适配,亦可以使用SPI扩展 配置加载流程文档地址：文档 Filter扩展服务提供方和服务消费方调用过程拦截，Dubbo 本身的大多功能均基于此扩展点实现，每次远 程方法执行，该拦截都会被执行，请注意对性能的影响。 异步——》同步 文档地址：文档","categories":[],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://huang-ding.github.io/hexo/tags/dubbo/"}]},{"title":"Dubbo多协议","slug":"dubbo-protocol","date":"2020-06-02T11:48:51.000Z","updated":"2020-06-02T12:47:08.775Z","comments":true,"path":"2020/06/02/dubbo-protocol/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/06/02/dubbo-protocol/","excerpt":"","text":"Dubbo多协议dubbo://协议Dubbo缺省协议，采用单一长链接和NIO异步通讯，适合于小数据量大并发的服务调用，以及消费者者机器数远大于服务提供者机器数的情况 反之，Dubb缺省协议不适合传送大数据量的服务，比如文件，视频等。除非请求量低 Transporter：mina，netty，grizzly Serialization：dubbo，hessian2，java，json Dispatcher：all，direct，message，execution，connection ThreadPool：fixed，cached 特性 链接个数：单链接 链接方式：长链接 传输协议：TCP 传输方式：NIO异步传输 序列化：Hessian二进制序列化 适用范围：传入传出参数数据包较小（建议小于100k），消费者比提供者数量多，单一消费者无法压满提供者，尽量不要使用dubbo协议传输大文件或者超大字符串 适用场景：常规远程服务方法调用 约束 参数以及返回值必须实现Serialzable接口 参数及返回值不能自定义实现List，Map,Number，Date，Calendar等接口，只能使用jdk自带的实现，因为hessian会做特殊的处理，自定义实现类中的属性和值会丢失 Hessian序列化，只传成员属性和值的类型，不传方法和静态变量，参数兼容情况见参考文档 配置协议： 1&lt;dubbo:protocol name=\"dubbo\" port=\"20880\" /&gt; 设置默认协议： 1&lt;dubbo:provider protocol=\"dubbo\" /&gt; 设置服务协议： 1&lt;dubbo:service protocol=\"dubbo\" /&gt; 多端口： 12&lt;dubbo:protocol id=\"dubbo1\" name=\"dubbo\" port=\"20880\" /&gt;&lt;dubbo:protocol id=\"dubbo2\" name=\"dubbo\" port=\"20881\" /&gt; 配置协议选项： 1&lt;dubbo:protocol name=“dubbo” port=“9090” server=“netty” client=“netty” codec=“dubbo” serialization=“hessian2” charset=“UTF-8” threadpool=“fixed” threads=“100” queues=“0” iothreads=“9” buffer=“8192” accepts=“1000” payload=“8388608” /&gt; 多连接配置： Dubbo 协议缺省每服务每提供者每消费者使用单一长连接，如果数据量较大，可以使用多个连接。 12&lt;dubbo:service connections=\"1\"/&gt;&lt;dubbo:reference connections=\"1\"/&gt; &lt;dubbo:service connections=&quot;0&quot;&gt; 或 &lt;dubbo:reference connections=&quot;0&quot;&gt; 表示该服务使用 JVM 共享长连接。缺省 &lt;dubbo:service connections=&quot;1&quot;&gt; 或 &lt;dubbo:reference connections=&quot;1&quot;&gt; 表示该服务使用独立长连接。 &lt;dubbo:service connections=&quot;2&quot;&gt; 或&lt;dubbo:reference connections=&quot;2&quot;&gt; 表示该服务使用独立两条长连接。 为防止被大量连接撑挂，可在服务提供方限制大接收连接数，以实现服务提供方自我保护。 1&lt;dubbo:protocol name=\"dubbo\" accepts=\"1000\" /&gt; dubbo.properties 配置： 1dubbo.service.protocol=dubbo rmi://hessian://http://webservice://thrift://mencached://redis://rest://grpc://参考文档：dubbo协议 性能测试报告：性能测试报告 多协议使用Dubbo允许配置多协议，在不同服务上支持不同协议或同一服务上同时支持多种协议 不同服务不同协议不同服务在性能上适用不同协议进行传输，比如大数据用短连接协议，小数据大并发用长连接协议 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dubbo=\"http://dubbo.apache.org/schema/dubbo\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd\"&gt; &lt;dubbo:application name=\"world\" /&gt; &lt;dubbo:registry id=\"registry\" address=\"10.20.141.150:9090\" username=\"admin\" password=\"hello1234\" /&gt; &lt;!-- 多协议配置 --&gt; &lt;dubbo:protocol name=\"dubbo\" port=\"20880\" /&gt; &lt;dubbo:protocol name=\"rmi\" port=\"1099\" /&gt; &lt;!-- 使用dubbo协议暴露服务 --&gt; &lt;dubbo:service interface=\"com.alibaba.hello.api.HelloService\" version=\"1.0.0\" ref=\"helloService\" protocol=\"dubbo\" /&gt; &lt;!-- 使用rmi协议暴露服务 --&gt; &lt;dubbo:service interface=\"com.alibaba.hello.api.DemoService\" version=\"1.0.0\" ref=\"demoService\" protocol=\"rmi\" /&gt; &lt;/beans&gt; 多协议暴露服务需要与 http 客户端互操作 12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dubbo=\"http://dubbo.apache.org/schema/dubbo\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd\"&gt; &lt;dubbo:application name=\"world\" /&gt; &lt;dubbo:registry id=\"registry\" address=\"10.20.141.150:9090\" username=\"admin\" password=\"hello1234\" /&gt; &lt;!-- 多协议配置 --&gt; &lt;dubbo:protocol name=\"dubbo\" port=\"20880\" /&gt; &lt;dubbo:protocol name=\"hessian\" port=\"8080\" /&gt; &lt;!-- 使用多个协议暴露服务 --&gt; &lt;dubbo:service id=\"helloService\" interface=\"com.alibaba.hello.api.HelloService\" version=\"1.0.0\" protocol=\"dubbo,hessian\" /&gt;&lt;/beans&gt; 协议配置项：dubbo:protocol","categories":[],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://huang-ding.github.io/hexo/tags/dubbo/"}]},{"title":"SPI机制","slug":"spi","date":"2020-05-25T11:45:04.000Z","updated":"2020-05-26T14:46:36.102Z","comments":true,"path":"2020/05/25/spi/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/25/spi/","excerpt":"","text":"SPI是什么SPI全称 Service Provider Interface ，是java提供用来被第三方实现或者扩展的API，可以用来启用框架和替换组件，让第三方提供自定义实现服务功能。 例如： JDBC驱动 加载Mysql ，Oracle ，SQL server Dubbo 扩展实现 java SPI约定 在工程的META-INF/service 目录下创建以接口名称的全路径文件，需要放在ClassPath中 在文件使用utf-8 写实现类的全路径 使用ServiceLoader.load(xxx.class)加载META-INF/service的实现类，此操作跨jar包执行 实现类需要一个无参构造方法 定义接口 12345678910111213package org.huangding.study.spi.javaspi;/** * @author huangding * @date 2020/5/25 22:20 */public interface Animal &#123; /** * @return 动物名称 */ String name();&#125; //实现接口 12345678910111213141516package org.huangding.study.spi.javaspi.impl;import org.huangding.study.spi.javaspi.Animal;/** * @author huangding * @date 2020/5/25 22:21 */public class Cat implements Animal &#123; @Override public String name() &#123; System.out.println(\"小猫猫\"); return \"小猫猫\"; &#125;&#125; 12345678910111213141516package org.huangding.study.spi.javaspi.impl;import org.huangding.study.spi.javaspi.Animal;/** * @author huangding * @date 2020/5/25 22:21 */public class Dog implements Animal &#123; @Override public String name() &#123; System.out.println(\"小狗狗\"); return \"小狗狗\"; &#125;&#125; META-INF/service 文件（org.huangding.study.spi.javaspi.Animal） 12#org.huangding.study.spi.javaspi.impl.Catorg.huangding.study.spi.javaspi.impl.Dog 使用 1234567891011121314import java.util.ServiceLoader;/** * @author huangding * @date 2020/5/25 22:26 */public class JavaSPITest &#123; public static void main(String[] args) &#123; //加载 META-INF/services文件数据 ServiceLoader&lt;Animal&gt; load = ServiceLoader.load(Animal.class); load.forEach(Animal::name); &#125;&#125; 输出 1小狗狗 Dubbo SPI机制Dubbo 并未使用 Java SPI，而是重新实现了一套功能更强的 SPI 机制。Dubbo SPI 的相关逻辑被封装在了 ExtensionLoader 类中，通过 ExtensionLoader，我们可以加载指定的实现类。Dubbo SPI 所需的配置文件需放置在 META-INF/dubbo 路径下，与 Java SPI 实现类配置不同，Dubbo SPI 是通过键值对的方式进行配置，这样我们可以按需加载指定的实现类。另外，在测试 Dubbo SPI 时，需要在提供的接口上标注 @SPI 注解. Dubbo SPI约定 在工程下创建META-INF/dubbo 使用ExtensionLoader.getExtensionLoader(xxxx.class)加载，也是跨包加载 实现和上面代码一致，接口添加 @SPI 注解 1234@SPIpublic interface Animal &#123; String name();&#125; 配置有点区别(META-INF/dubbo.com.alibaba.dubbo.demo.provider.spi.Animal) 12cat&#x3D;com.alibaba.dubbo.demo.provider.spi.Catdog&#x3D;com.alibaba.dubbo.demo.provider.spi.Dog 使用有点区别 123456789public class DubboSPITest &#123; public static void main(String[] args) &#123; ExtensionLoader&lt;Animal&gt; extensionLoader = ExtensionLoader.getExtensionLoader(Animal.class); //获取对应的实现 Animal dog = extensionLoader.getExtension(\"cat\"); System.out.println(dog.name()); &#125;&#125; doubbo spi 官方简介 SpringBoot SPI机制springboot start 使用SpringFactoriesLoader代替ServiceLoader，使用META-INF/spring.factories代替META-INF/service 1234567891011121314151617public static &lt;T&gt; List&lt;T&gt; loadFactories(Class&lt;T&gt; factoryClass, @Nullable ClassLoader classLoader) &#123; Assert.notNull(factoryClass, \"'factoryClass' must not be null\"); ClassLoader classLoaderToUse = classLoader; if (classLoaderToUse == null) &#123; classLoaderToUse = SpringFactoriesLoader.class.getClassLoader(); &#125; List&lt;String&gt; factoryNames = loadFactoryNames(factoryClass, classLoaderToUse); if (logger.isTraceEnabled()) &#123; logger.trace(\"Loaded [\" + factoryClass.getName() + \"] names: \" + factoryNames); &#125; List&lt;T&gt; result = new ArrayList&lt;&gt;(factoryNames.size()); for (String factoryName : factoryNames) &#123; result.add(instantiateFactory(factoryName, factoryClass, classLoaderToUse)); &#125; AnnotationAwareOrderComparator.sort(result); return result; &#125; spring boot读取properties文件spring.factories","categories":[],"tags":[{"name":"SPI","slug":"SPI","permalink":"https://huang-ding.github.io/hexo/tags/SPI/"}]},{"title":"springboot 杂记","slug":"spring-boot-enable-auto","date":"2020-05-17T09:13:58.000Z","updated":"2020-05-19T11:58:43.095Z","comments":true,"path":"2020/05/17/spring-boot-enable-auto/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/17/spring-boot-enable-auto/","excerpt":"","text":"@SpringBootApplication 注解这个注解相当于三个注解的功能集成 @Configuration：允许在Spring中注册额外的bean或配置 @EnableAutoConfiguration：启动Spring Boot的自动bean加载机制 @ComponentScan：在应用程序所在的包启用扫描,若未指定basePackages属性则只扫描同目录及以下的包 零Spring配置文件SpringBoot中建议放弃通过XML定义Spring 应用程序，推荐在代码类上面通过@Configuration实现配置，如果有需要还可以通过@ImportResource导入XML配置文件 个性化加载配置 节选spring boot autoconfigure 代码 使用个性化配置加载DataSource 配置 123456789@Configuration(proxyBeanMethods = false) @Conditional(PooledDataSourceCondition.class) @ConditionalOnMissingBean(&#123; DataSource.class, XADataSource.class &#125;) @Import(&#123; DataSourceConfiguration.Hikari.class, DataSourceConfiguration.Tomcat.class, DataSourceConfiguration.Dbcp2.class, DataSourceConfiguration.Generic.class, DataSourceJmxConfiguration.class &#125;) protected static class PooledDataSourceConfiguration &#123; &#125; 外部参数配置信息加载spring应用程序可以通过属性文件，yaml文件，环境变量和命令行行为参数等方式的外部化参数配置 启动时命令行传参 java -jar app.jar –name=’test’ java -jar spingboot-demo.jar –spring.profiles.active=”test” Spring Boot 配置信息中的特殊值：SPRING_APPLICATION_JSON=’{name:test}’ java -jar spingboot-demo.jar –SPRING_APPLICATION_JSON=”{&quot;spring.profiles.active&quot;:&quot;test&quot;,xxx:xxx}” 如果是web应用，可以读取ServletConfig init 参数、或ServletContext init参数 JNDI属性来自java:com/env 操作系统环境变量 配置文件application.properties,application-{profile}.properties,application.yml,application-{profile}.yml spring.profiles.active 环境变量 @PropertySource注解导入的配置：@PropertySource(value=”person.properties”) 程序入口通过SpringApplication.setDefaultProperties方法设定的参数配置 123456789101112//自定义参数@SpringBootApplicationpublic class StudyApplication &#123; public static void main(String[] args) &#123; SpringApplication springApplication = new SpringApplication(StudyApplication.class); Properties properties = new Properties(); properties.setProperty(\"name\",\"test\"); springApplication.setDefaultProperties(properties); springApplication.run(args); &#125;&#125; 环境化配置-profileprofile 是什么机制 Spring配置文件提供了一种隔离应用程序的方法，使其在特定的环境中可以使用。 可以通过profile指定Bean的应用环境（dev，test,prod） 可通过profile指定不同的环境配置参数值 指定profile 通过环境配置参数spring.profiles.active来指定应用启用的profile,默认default 在环境变量中指出，jvm参数，命令行参数，application文件中设定 代码指定：springApplication.setAdditionalProfiles(“dev”) 使用 Configuration类或者Bean方法上，添加@Profile(“dev”)注解 12345@Bean@Profile(\"dev\")public void print()&#123; System.out.println(\"测试环境\");&#125; 配置文件：xxx 配置文件配置文件可以放什么地方 当前项目运行盘符/config文件夹下面:file:./config/ 当前项目运行的目录下面（命令执行的所在目录）:file:./ classpath下面的config文件夹：classpath:/config classpath的根目录（常用）:classpath:/ 文件按照优先级排序，排在上面的会覆盖优先级低的配置 自定义配置名称和存储路径 spring.config.name=properties-file-name spring.config.location=classpath:/config/,file:./config(从右到左反序搜索) 必须将它们定义成环境属性，通常是操作系统的环境变量，JVM参数或者命令行参数 配置文件格式支持两种配置格式：.properites,.yml ymal基础语法： 大小写敏感 使用空格进行缩进（不要用tab），同级左侧对其 map键值对使用“ ：”分割 list列表使用 “ - ”表示 1234key: valuelist: - demo1 - demo2 properties实例：spring.datasource.username=test 参数使用 使用@Value(“name”)注解，将指定参数配置注入到属性 12@Value(\"$&#123;host.url:url&#125;\")provide String host; 注入Environment对象 123456@Autowiredprivate Environment environment; public String getName()&#123; return environment.getProperty(\"name\");&#125; 使用注解@ConfigurationProperties(prefix=”test”),将注解加到指定类上，spring会为实例对象的对应属性进行赋值，属性需要有getters和setters方法 123456@Component@ConfigurationProperties(prefix = \"test\")@Datapublic class TestConfig &#123; private String name;&#125; Starter快速集成作用启动器（Starter）包含许多依赖项，这些依赖项使项目快速启动和运行所需的依赖项； 例如：通过配置spring-boot-starter-data-redis，可以快捷使用spring对redis访问 命名规范官方开发的starter遵循类似命名的模式；spring-boot-starter-* 第三方starter命名规范应当遵循third party porject-spring-boot-starter 常用的starter spring-boot-starter-jdbc spring-boot-starter-data-redis spring-boot-starter-web spring-boot-starter-actuator 自研Starter的步骤 建maven工程 引入spring-boot-start ，spring-boot-autoconfiguration，第三方jar 123456789101112131415161718&lt;!--引入spring boot starter--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.2.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;version&gt;2.2.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;version&gt;2.2.1.RELEASE&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 如果需要生成配置元信息(spring-configuration-metadata.json)，加入spring-boot-configuration-processor依赖 编写配置类 12345678910111213@ConfigurationProperties(\"org.hd.my.starter\")public class MyStarterProperties &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 1234567891011@Configuration@EnableConfigurationProperties(MyStarterProperties.class)public class MyStarterAutoConfiguration &#123; @Bean public MyStarter getMyStarter(MyStarterProperties myStarterProperties) &#123; MyStarter myStarter = new MyStarter(); myStarter.setName(myStarterProperties.getName()); return myStarter; &#125;&#125; 配置发现配置文件：META-INF/spring.factories 123# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.hd.my.starter.spring.boot.autoconfiguration.MyStarterAutoConfiguration 打包发布 12345&lt;dependency&gt; &lt;groupId&gt;org.hd&lt;/groupId&gt; &lt;artifactId&gt;my-starter-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;","categories":[],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://huang-ding.github.io/hexo/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://huang-ding.github.io/hexo/tags/SpringBoot/"}]},{"title":"rpc demo 杂记","slug":"rpc-demo-coding","date":"2020-05-16T13:01:03.000Z","updated":"2020-05-16T13:05:59.612Z","comments":true,"path":"2020/05/16/rpc-demo-coding/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/16/rpc-demo-coding/","excerpt":"","text":"demo代码实现 https://github.com/huang-ding/rpc-demo 原图设计 https://www.processon.com/view/link/5ebe814ee401fd16f43c86e3","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"rpc","slug":"rpc","permalink":"https://huang-ding.github.io/hexo/tags/rpc/"}]},{"title":"RPC理论","slug":"rpc-bash","date":"2020-05-14T12:06:07.000Z","updated":"2020-05-14T13:24:10.933Z","comments":true,"path":"2020/05/14/rpc-bash/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/14/rpc-bash/","excerpt":"","text":"什么是RPCremote procedure call (RPC)：远程过程调用。 业务处理，任务计算，用本地方法一样调用远程的过程。 RPC采用Client-Server结构，通过request-response消息模式实现 RPC和RMI有什么区别 RMI: 远程方法调用(Remote Method Invocation)，它支持存储于不同地址空间的程序级对象之间彼此进行通信，实现远程对象之间的无缝远程调用。Java RMI: 用于不同虚拟机之间的通信，这些虚拟机可以在不同的主机上、也可以在同一个主机上；一个虚拟机中的对象调用另一个虚拟上中的对象的方法，只不过是允许被远程调用的对象要通过一些标志加以标识；是oop领域中RPC的一直具体实现 webservice,restfull等类似的也是RPC，仅消息的组织方式和协议不同 远程过程和本地的区别 速度相对慢，需要网络通信 可靠性减弱，比如网络不同 RPC的流程环节 客户端处理过程调用Client stub 如调用本地方法一般，传递参数； Client stub 将参数编组为消息，通过系统调用向服务端发送消息； 客户端本地操作系统将消息从客户端发送到服务器; 服务端收到数据包传递给Server stub; Server stub 解组消息为参数； Server stub 再调用服务端的过程，过程执行的结果响应给客户端 流程问题 Client stub，Server stub 的研发； 参数编组和解组； 消息如何发送； 过程结果的数据表示，异常如何处理； 如何实现安全的访问控制，防止服务攻击； PRC协议RPC调用过程中需要将参数编组为消息进行发送，接收方需要解组消息为参数，过程处理结果同样需要经过编组，解组。消息由哪些部分构成（消息头，消息体等）以及消息的表示形式(xml,json等)构成了消息协议 RPC调用过程中采用的消息协议称为RPC协议 RPC协议规定请求，响应消息的格式 在TCP上可以选用或自定义消息协议来完成RPC消息交互 我们可以选用通用的标准协议（http，https）也可以更据自身需求自定义自己的消息协议 常见的RPC协议 RPC框架封装好参数编组，消息解组，底层网络通信的RPC程序开发框架，带来的便捷是可以直接在其基础上只需专注于过程代码编写。 传统的webservice框架：Apache CXF，Apache Axis2，java自带的JAX-WS等。webservcie框架大多基于标准的SOAP协议 新兴的微服务框架：Dubbo，spring cloud，Apache Thrift 等 gRPC：一开始由 google 开发，是一款语言中立、平台中立、开源的远程过程调用(RPC)系统。 使用RPC的好处 服务化 可重用 系统交互调用 RPC相关术语 Client，Server，calls，replies，service，programs，procedures，version，marshalling（编组），unmarshalling(解组) 一个网络服务由多个远程程序集构成 一个远程程序实现一个或者多个程序过程 过程，过程的参数，过程的结果在程序协议说明中定义说明 为兼容程序协议变更过，一个服务端可能支持多个版本的远程程序","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"rpc","slug":"rpc","permalink":"https://huang-ding.github.io/hexo/tags/rpc/"}]},{"title":"分布式系统设计重要理论","slug":"distributed-core-theory","date":"2020-05-13T14:55:18.000Z","updated":"2020-05-13T14:55:24.377Z","comments":true,"path":"2020/05/13/distributed-core-theory/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/13/distributed-core-theory/","excerpt":"","text":"分布式基石-CAP定理CPA定理（CAP theorem），又称布鲁尔定理（Eric Brewer），1998年第一次提出。 最初提出是指分布式数据存储不可能同时提供以下三种保证中的两种以上 一致性（Consistency）：每次读取收到的消息是最新的。 可用性（Availability）：每个请求都会收到(非错误)响应。 分区容错性（Partition tolerance）：尽管节点之间的网络不通导致分区，系统可以继续运行 事实上，不仅仅是分布式数据存储，所有的分布式系统都必须在CAP这三点之间权衡 一般分布式系统需要保证分区容错性，在此基础上进行C和A的取舍 保证分区容错性可通过多个节点保证，但是会出现有状态服务的一致性问题，当保证节点之间状态数据一致时，可能导致系统短暂不可用，保证一致性丧失可用性，网络同步数据需要一定时间； 也有满足CAP的应用出现，实现CAP的一部分，舍弃一部分，每个地方牺牲一点，做到每个的部分满足，在业务代码做细节控制 数据一致性模型如果数据读取，写入，更新的结果是可以预测的，我们说它遵循数据一致性模型 严格一致性（Strict Consistency）（强）不论在那个节点，看到的资源都是统一的结果 顺序一致性（Sequenctial Consistency）（弱）节点的数据变动和操作的顺序保持一致，类似ZK 最终一致性（Eventual Consistency）（弱）所有数据副本最终都会变的一致，但是不保证执行顺序 强弱划分比较粗狂，但是容易理解，并发编程和分布式计算领域有更多细分模型，如会话一致性等 分布式算法基石：Paxos算法 BASE理论BASE是Basically Available（基本可用）Soft state（软状态）和Eventual consistency（最终一致性），三个短语缩写 基本可用：可能是部分功能不可用或者响应时间延长，如熔断降级处理 软状态：不同系统/节点之间，数据存在过度状态，前提是数据状态不影响业务流程，且时间有限 最终一致：经过系统内部协调机制，最终所有节点保存一致（分布式系统的一致性不一定是指数据保持一致） BASE是指导理论用于设计分布式系统，兼顾了CAP三者，降低了一致性要求，需要友好的应用程序协调机制 业务系统设计的原则墨菲定律墨菲定律（Murphy`s law）是一种心理学效应，由爱德华.墨菲提出 原句：如果有两种或两种以上的方式做某件事情，而其中一种选择方式将导致灾难，则必定有人会做出这种选择。 本质：如果事情有变坏的可能，不管可能性多小，它都会发生 如：觉得功能很简单，很快弄完，但是时间一拖再拖；觉得某个技术栈很简单，所以了解一部分之后马上就想来用，结果付出更多的时间找bug和学习；觉得某段代码复杂可能会有问题，觉得TMD居然真的有问题；觉得程序性能有问题，结果又TMD出问题。。。类似 系统设计和架构上的理解： 任何事情都没有表面看起来那么简单 所有的事情都会比你预计的时间长 会出错的事情总会出错 如果你担心每个bug出现问题，那么它就更加可能发生 康威定律设计系统的架构受制于产生这些设计的组织的沟通结构———-Conways Law 系统设计上的思考： 系统架构是公司组织架构的反映 应该按照业务闭环进行系统拆分/组织架构划分，实现闭环/高内聚/低耦合，减少沟通成本（在合适时机进行系统拆分，不要一开始就把系统服务拆分的非常细，虽然闭环，但是每个人维护的系统多，维护成本高） 如果沟通出现问题，那么应该考虑进行系统和组织架构的调整 时间再多一件事情也不可能做的完美，但总有时间做完一件事情","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"zookeeper集群","slug":"distributed-zk-cluster","date":"2020-05-12T13:27:55.000Z","updated":"2020-05-12T14:55:40.400Z","comments":true,"path":"2020/05/12/distributed-zk-cluster/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/12/distributed-zk-cluster/","excerpt":"","text":"集群特点 可靠的ZooKeeper服务 只要集群的大多数(50%)都准备好了，就可以使用服务 容错集群设置至少需要3台服务器，最好使奇数个服务器(3台和4台选举方式一致) 建议每个服务运行单独机器 配置123456789101112//心跳时间(毫秒)tickTime=2000//集群中follower服务器和Leader服务器之间完成初始化同步连接时最大心跳数。如果zk集群数量确实很大，同步时间会变长，因此可以适当调大该参数initLimit=10//集群的follower服务器与leader服务器之间请求和应答之间能够容忍最大心跳数syncLimit=5dataDir=F:/zk/colony/zk1/dataclientPort=2182//集群节点 id:通过dataDir目录创建myid的文件，一行文本只包含机器id，来为每台机器赋予一个服务id(1-255)之间；两个端口号，第一个跟随着用来连接到领导者，第二个用来选举领导者server.1=localhost:2881:3881server.2=localhost:2882:3882server.3=localhost:2883:3883 集群所有节点都可以提供服务，客户端链接时，链接串可以指定多个或全部集群节点地址，当一个节点不通时客户端自动切换 1ZkClient client = new ZkClient(\"localhost:2181,localhost:2182,localhost:2183\"); 启动日志解析12//启动的配置信息2020-05-12 20:07:49,662 [myid:2] - INFO [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@329] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 clientPortListenBacklog -1 datadir F:\\zk\\colony\\zk2\\data\\version-2 snapdir F:\\zk\\colony\\zk2\\data\\version-2 12//Ladder选举的方式2020-05-12 20:08:09,028 [myid:1] - INFO [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):FastLeaderElection@944] - New election. My id = 1, proposed zxid=0x100000013 12//当前节点的状态2020-05-12 20:08:09,101 [myid:1] - INFO [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):Follower@75] - FOLLOWING - LEADER ELECTION TOOK - 74 MS 12345//加入节点同步节点数据2020-05-12 20:08:09,114 [myid:2] - INFO [LearnerHandler-/127.0.0.1:57949:LearnerHandler@504] - Follower sid: 1 : info : localhost:2881:3881:participant2020-05-12 20:08:09,159 [myid:2] - INFO [LearnerHandler-/127.0.0.1:57949:ZKDatabase@345] - On disk txn sync enabled with snapshotSizeFactor 0.332020-05-12 20:08:09,160 [myid:2] - INFO [LearnerHandler-/127.0.0.1:57949:LearnerHandler@800] - Synchronizing with Learner sid: 1 maxCommittedLog=0x0 minCommittedLog=0x0 lastProcessedZxid=0x300000000 peerLastZxid=0x100000013 123//同步最新zxid日志[QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):Follower@170] - Got zxid 0x300000001 expected 0x1[SyncThread:1:FileTxnLog@284] - Creating new log file: log.300000001 集群监控 四字监控 JMX ZAB协议作为重要的分布式协调服务，如何保证集群数据的一致性？ 读可以所有节点读取，但是写需要先转发给leader，进行协调 所有事物请求转发给leader leader分配全局单调递增事物id(Zxid),广播事物提议 Follower处理提议，做出反馈 leader收到半数节点反馈，广播Commit Leader做出响应 返回给客户端 保证有序性，从而保证数据顺序一致性 ZAB协议-崩溃恢复Leader服务器出现奔溃，或者出现半数Follower与Leader断开联系，那么就会进入崩溃恢复模式 ZAB协议规定如果一个事物Proposal在一台机器上被处理成功(Commit)，那么应该在所有的机器上都被处理成功，哪怕机器出现奔溃故障 ZAB协议确保那些已经在Leader服务器提交的事物最终被所有服务器提交 ZAB协议确保丢弃那些只在Leader服务器提出(未ACK,未Commit)的事物 ZAB协议需要设计的选举算法应该满足： 确保提交已经被leader提交的事物Proposal，同时丢弃已经被跳过的事物Proposal 让Leader选举算法那个保证新选举的Leader服务器拥有集群最高的ZXID的事物Proposal，可以保证最高新选举的Leader一定具有所有已经提交的提按 让具有最高编号ZXID事物Proposal成为Leader可以省去Leader服务器检查Proposal的提交和丢弃工作这一步 崩溃恢复期间ZK无法提供服务 ZAB协议-数据同步Leader选举出来后，需要完成Follower与Leader的数据进行同步，当半数同步完成，可以开始提供服务 Leader服务器会为每个Follower服务器都准备一个队列，将每个Follower没有同步的事物已Proposal消息的形式逐个发送，并紧跟一个Commit消息，以表示事物已提交 Follower服务器同步事物后，Leader服务器就会将该Follower服务器加入真正的可用Follower服务器列表中，开始其他流程 ZAB协议-丢弃事物Proposal处理在ZAB协议的事物编号ZXID设计中，ZXID是一个64位数字 低32位是一个简答的单调递增计数器，Leader服务器产生一个新的Proposal的时候进行加一操作； 高32位代表Leader周期纪元编号，每次新选举Leader服务器，进行加1，同时低32位归0 基于这样的策略，当一个包含了上一个leader周期尚未提交过的事物Proposal服务器启动加入到集群时，发现此时集群已经存在leader，将自身已Follower角色链接上Leader服务器之后，Leader会更据自己服务器最后提交的Proposal和Follower服务器的Proposal进行比较，发现Follower中有上个周期的事物时，leader会要求follower进行回退操作(回退到一个确实被集群半数提交的最新事物Proposal) 选举对于选举算法的要求 选出的Leader节点上要求当前集群最高的zxid 过半节点数同意 内置实现的选举算法 LeaderElection FastLeaderElection AuthFastLeaderElection 选举机制中的概念 服务器id myid 事物id，服务器存放最大的zxid 逻辑时钟 选举状态 Looking ，竞选状态 Following 随从状态，同步Leader状态，参与投票 Observing 观察状态，同步Leader状态 不参与投票 Leading 领导者状态 选举算法 每个服务器均发起选举自己为Leader的投票(自己的给自己) 其他服务器收到投票邀请，对比事物id，比自己大的，小则不投，相等对比服务器id，大投，小不投 发起者收到大家投票反馈后，看投票数是否大于集群半数，大于则担任Leader，为超过继续发起投票 胜出条件：赞成数大于半数胜出 ZK与CAPzk保证了C(顺序一致性)和P(分区容错性)，牺牲一部分可用性(崩溃恢复无法提供服务)","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"zk","slug":"zk","permalink":"https://huang-ding.github.io/hexo/tags/zk/"}]},{"title":"zookeeper典型应用场景","slug":"distributed-zk-apply","date":"2020-05-08T11:59:13.000Z","updated":"2020-05-10T14:20:59.807Z","comments":true,"path":"2020/05/08/distributed-zk-apply/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/08/distributed-zk-apply/","excerpt":"","text":"数据发布订阅(配置中心)何为配置中心：解决系统参数配置，动态该参通知 用zookeeper实现配置中心 znode能存储数据 一个配置项一个znode 一个配置文件一个znode Watch能监听数据变化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * zk 配置中心demo */public class ConfigCenterDemo &#123; //普通配置 public static void put2Zk(String key, String value) &#123; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); if (client.exists(key)) &#123; client.writeData(key, value); &#125; else &#123; client.createPersistent(key, value); &#125; client.close(); &#125; //文件配置 public static void put2ZKFile(String key, File file) throws IOException &#123; FileInputStream fileInputStream = new FileInputStream(file); byte[] data = new byte[(int) file.length()]; fileInputStream.read(data); fileInputStream.close(); ZkClient client = new ZkClient(\"localhost:2181\"); //选择对应的序列号实现 client.setZkSerializer(new BytesPushThroughSerializer()); if (client.exists(key)) &#123; client.writeData(key, data); &#125; else &#123; client.createPersistent(key, data); &#125; client.close(); &#125; public static void get(String key) &#123; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); String value = client.readData(key); System.out.println(\"从zk读到配置\" + key + \"的值为：\" + value); //监听配置变化 client.subscribeDataChanges(key, new IZkDataListener() &#123; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; System.out.println(\"配置更新：\" + data); &#125; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; System.out.println(\"配置已删除\"); &#125; &#125;); // 这里只是为演示实时获取到配置值更新而加的等待。实际项目应用中根据具体场景写（可用阻塞方式） try &#123; Thread.sleep(5 * 60 * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; client.close(); &#125; public static void main(String[] args) throws IOException &#123;// ConfigCenterDemo.put2Zk(\"/hd\",\"666\");// ConfigCenterDemo.get(\"/hd\"); File f = new File(ConfigCenterDemo.class.getResource(\"/application.yml\").getFile()); ConfigCenterDemo.put2ZKFile(\"/hdFile\",f); ConfigCenterDemo.get(\"/hdFile\"); &#125;&#125; 命名服务何为命名服务：服务之间通过服务名称可以动态获取到服务调用地址 1234567891011121314151617181920212223242526public class NamespacesZk &#123; public static void registered(String serverName, String serverAddress) &#123; ConfigCenterDemo.put2Zk(serverName, serverAddress); &#125; public static void main(String[] args) &#123; //服务B注册 registered(\"/serverB\", \"http://www.baidu.com\"); //服务A监听 ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); String addressB = client.readData(\"/serverB\"); client.subscribeDataChanges(\"/serverB\", new IZkDataListener() &#123; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; System.out.println(\"serverB服务的地址变化\"); &#125; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; &#125; &#125;); &#125;&#125; Master选举何为Master选举：分布式服务主从结构一主多从，当主节点不可用时，自动选取出新的子节点 zookeeper实现Master选举 使用临时节点，争取创建主节点 使用临时节点防止主节点挂掉无法通知其他节点 创建servers节点，创建临时/临时顺序子节点，管理当前可用节点信息 使用临时顺序节点，可以使用最小节点当做主节点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135/** * zk Master 选举 demo */public class MasterElectionDemo &#123; static class Server &#123; //集群名称 节点名称 节点地址 private String clusterName, nodeName, nodeAddress; private String masterInfo; //当前节点目录 数据 private final String path, value; //可用节点信息 private String serversPath; private List&lt;String&gt; servers; public Server(String clusterName, String nodeName, String nodeAddress) &#123; this.clusterName = clusterName; this.nodeName = nodeName; this.nodeAddress = nodeAddress; this.path = \"/\" + this.clusterName + \"Master\"; this.serversPath = \"/\" + this.clusterName + \"servers\"; this.value = \"nodeName:\" + nodeAddress + \" nodeAddress:\" + nodeAddress; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); new Thread(new Runnable() &#123; @Override public void run() &#123; //节点注册 electionMaster(client); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; //当前存活节点数据获取 electionServers(client); &#125; &#125;).start(); &#125; private void createServers(ZkClient client) &#123; if (!client.exists(serversPath)) &#123; client.createPersistent(serversPath); &#125; try &#123; client.createEphemeral(serversPath + \"/\" + nodeName, nodeAddress); &#125; catch (ZkNodeExistsException e) &#123; client.writeData(serversPath + \"/\" + nodeName, nodeAddress); &#125; &#125; private void electionServers(ZkClient client) &#123; //阻塞等待 CountDownLatch latch = new CountDownLatch(1); IZkChildListener iZkChildListener = new IZkChildListener() &#123; @Override public void handleChildChange(String parentPath, List&lt;String&gt; currentChilds) throws Exception &#123; List&lt;String&gt; children = client.getChildren(parentPath); servers = new ArrayList&lt;&gt;(); for (String child : children) &#123; System.out.println(\"子节点消息：\" + child); servers.add(client.readData(parentPath + \"/\" + child)); &#125; System.out.println(\"存活子节点：\" + servers); latch.countDown(); &#125; &#125;; client.subscribeChildChanges(serversPath, iZkChildListener); try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; electionServers(client); &#125; private void electionMaster(ZkClient client) &#123; //创建当前存活节点信息 createServers(client); try &#123; //创建Master节点 client.createEphemeral(path, value); System.out.println(value + \"创建节点成功，成为Master\"); &#125; catch (ZkNodeExistsException e) &#123; &#125; masterInfo = client.readData(path); System.out.println(\"当前主节点为:\" + masterInfo); //阻塞等待 CountDownLatch latch = new CountDownLatch(1); IZkDataListener iZkDataListener = new IZkDataListener() &#123; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; &#125; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; //节点删除 进行主节点选举 唤醒 latch.countDown(); &#125; &#125;; //监听主节点 client.subscribeDataChanges(path, iZkDataListener); //阻塞 等待唤醒 if (client.exists(path)) &#123; try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //取消监听 client.unsubscribeDataChanges(path, iZkDataListener); //递归调用 进行下一次选举 electionMaster(client); &#125; &#125; public static void main(String[] args) &#123; // 测试时，依次开启多个Server实例java进程，然后停止获取的master的节点，看谁抢到Master// Server s = new Server(\"cluster1\", \"server1\", \"192.168.1.11:8991\"); Server s = new Server(\"cluster1\", \"server2\", \"192.168.1.11:8992\");// Server s = new Server(\"cluster1\", \"server3\", \"192.168.1.11:8993\");// Server s = new Server(\"cluster1\", \"server4\", \"192.168.1.11:8994\"); &#125;&#125; 分布式队列zookeeper实现分布式队列：使用顺序节点，入队(创建顺序节点)，出队(消费者取出所有子节点，移除最小号节点) 无界队列 有界队列 分布式锁，判断是否到达上限，防止超界 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * zk 分布式队列 简易demo * * @author huangding * @date 2020/5/10 20:18 */public class ZKQueue &#123; private final String queueName; private final int size; public ZKQueue(String queueName) &#123; this.queueName = queueName; size = Integer.MAX_VALUE; &#125; public ZKQueue(String queueName, int size) &#123; this.queueName = queueName; this.size = size; &#125; public void add(String value) &#123; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); try &#123; client.createPersistent(\"/\" + queueName); &#125; catch (ZkNodeExistsException e) &#123; &#125; List&lt;String&gt; children = client.getChildren(\"/\" + this.queueName); int childrenSize = children.size(); //TODO 正式情况需要使用分布式锁 ，有界队列 if (childrenSize &gt;= this.size) &#123; //队列已满 return; &#125; client.createPersistentSequential(\"/\" + queueName + \"/queue\", value); client.close(); System.out.println(\"入队：\" + value); &#125; public String poll() &#123; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); //TODO 正常情况消费需要加锁防止重复消费 List&lt;String&gt; children = client.getChildren(\"/\" + this.queueName); if (null == children || children.size() == 0) &#123; return null; &#125; Collections.sort(children); //移除最小节点 String s = children.get(0); String data = client.readData(\"/\" + this.queueName + \"/\" + s); client.delete(\"/\" + this.queueName + \"/\" + s); client.close(); System.out.println(\"出队：\" + data); return data; &#125; public static void main(String[] args) &#123; ZKQueue zkQueue = new ZKQueue(\"hdQueue\"); zkQueue.add(\"1\"); zkQueue.add(\"2\"); zkQueue.add(\"3\"); zkQueue.add(\"4\"); boolean f = true; do &#123; String poll = zkQueue.poll(); if (null == poll) &#123; f = false; &#125; &#125; while (f); &#125;&#125; 分布式锁zookeeper实现分布式锁： 挣抢锁，利用临时节点，原理：节点不可重名+watch;缺点：惊群效应 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143/** * zk 挣抢锁 实现demo 缺点惊群效应 */public class ZKDistributeLock implements Lock &#123; private final String lockPath; private ZkClient client; /** * 重入次数 */ private ThreadLocal&lt;Integer&gt; reentrantCount = new ThreadLocal&lt;&gt;(); public ZKDistributeLock(String lockPath) &#123; this.lockPath = \"/\" + lockPath; ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); this.client = client; &#125; @Override public void lock() &#123; if (!tryLock()) &#123; //阻塞 waitForLock(); //再次尝试 lock(); &#125; &#125; private void waitForLock() &#123; CountDownLatch latch = new CountDownLatch(1); IZkDataListener listener = new IZkDataListener() &#123; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; &#125; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; //锁节点删除，开始抢锁 System.out.println(\"收到节点删除\"); latch.countDown(); &#125; &#125;; //watch监听 client.subscribeDataChanges(lockPath, listener); //阻塞 if (client.exists(lockPath)) &#123; try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 取消监听事件，进行抢锁 client.unsubscribeDataChanges(lockPath, listener); &#125; @Override public void lockInterruptibly() throws InterruptedException &#123; &#125; @Override public boolean tryLock() &#123; //可重入判断 if (reentrantCount.get() != null) &#123; Integer count = reentrantCount.get(); if (count &gt; 0) &#123; reentrantCount.set(++count); return true; &#125; &#125; try &#123; //创建锁的临时节点 client.createEphemeral(lockPath); reentrantCount.set(1); return true; &#125; catch (ZkNodeExistsException e) &#123; return false; &#125; &#125; @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; return false; &#125; @Override public void unlock() &#123; //判断重入锁释放 if (reentrantCount != null) &#123; Integer count = reentrantCount.get(); if (count &gt; 1) &#123; reentrantCount.set(--count); &#125; else &#123; reentrantCount.set(null); &#125; &#125; //删除节点，释放锁 client.delete(lockPath); &#125; @Override public Condition newCondition() &#123; return null; &#125; public static void main(String[] args) &#123; int cbCount = 50; //模拟高并发 CyclicBarrier cb = new CyclicBarrier(cbCount); for (int i = 0; i &lt; cbCount; i++) &#123; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + \"线程已准备\"); try &#123; cb.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; ZKDistributeLock lock = new ZKDistributeLock(\"testLock\"); try &#123; lock.lock(); System.out.println(Thread.currentThread().getName() + \"或得锁\"); &#125; finally &#123; lock.unlock(); &#125; &#125;).start(); &#125; &#125;&#125; 取号，利用临时顺序节点来实现分布式锁；，获取锁：取排队号（创建自己的临时顺序节点），然后判断自己是否是最小号，如是，则获得锁；不是，则注册前一节点的watcher,阻塞等待，释放锁：删除自己创建的临时顺序节点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178/** * zk 利用临时顺序节点来实现分布式锁 实现demo */public class ZKDistributeImproveLock implements Lock &#123; /* * 利用临时顺序节点来实现分布式锁 * 获取锁：取排队号（创建自己的临时顺序节点），然后判断自己是否是最小号，如是，则获得锁；不是，则注册前一节点的watcher,阻塞等待 * 释放锁：删除自己创建的临时顺序节点 */ private String lockPath; private ZkClient client; private ThreadLocal&lt;String&gt; currentPath = new ThreadLocal&lt;&gt;(); private ThreadLocal&lt;String&gt; beforePath = new ThreadLocal&lt;&gt;(); // 锁重入计数 private ThreadLocal&lt;Integer&gt; reentrantCount = new ThreadLocal&lt;&gt;(); public ZKDistributeImproveLock(String lockPath) &#123; super(); this.lockPath = lockPath; client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); if (!this.client.exists(lockPath)) &#123; try &#123; this.client.createPersistent(lockPath); &#125; catch (ZkNodeExistsException e) &#123; &#125; &#125; &#125; @Override public boolean tryLock() &#123; if (this.reentrantCount.get() != null) &#123; int count = this.reentrantCount.get(); if (count &gt; 0) &#123; this.reentrantCount.set(++count); return true; &#125; &#125; if (this.currentPath.get() == null) &#123; currentPath.set(this.client.createEphemeralSequential(lockPath + \"/\", \"aaa\")); &#125; // 获得所有的子 List&lt;String&gt; children = this.client.getChildren(lockPath); // 排序list Collections.sort(children); // 判断当前节点是否是最小的 if (currentPath.get().equals(lockPath + \"/\" + children.get(0))) &#123; this.reentrantCount.set(1); return true; &#125; else &#123; // 取到前一个 // 得到字节的索引号 int curIndex = children.indexOf(currentPath.get().substring(lockPath.length() + 1)); beforePath.set(lockPath + \"/\" + children.get(curIndex - 1)); &#125; return false; &#125; @Override public void lock() &#123; if (!tryLock()) &#123; // 阻塞等待 waitForLock(); // 再次尝试加锁 lock(); &#125; &#125; private void waitForLock() &#123; CountDownLatch cdl = new CountDownLatch(1); // 注册watcher IZkDataListener listener = new IZkDataListener() &#123; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; System.out.println(\"-----监听到节点被删除\"); cdl.countDown(); &#125; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; &#125; &#125;; client.subscribeDataChanges(this.beforePath.get(), listener); // 怎么让自己阻塞 if (this.client.exists(this.beforePath.get())) &#123; try &#123; cdl.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 醒来后，取消watcher client.unsubscribeDataChanges(this.beforePath.get(), listener); &#125; @Override public void unlock() &#123; // 重入的释放锁处理 if (this.reentrantCount.get() != null) &#123; int count = this.reentrantCount.get(); if (count &gt; 1) &#123; this.reentrantCount.set(--count); return; &#125; else &#123; this.reentrantCount.set(null); &#125; &#125; // 删除节点 this.client.delete(this.currentPath.get()); &#125; @Override public void lockInterruptibly() throws InterruptedException &#123; // TODO Auto-generated method stub &#125; @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; // TODO Auto-generated method stub return false; &#125; @Override public Condition newCondition() &#123; // TODO Auto-generated method stub return null; &#125; public static void main(String[] args) &#123; // 并发数 int currency = 50; // 循环屏障 CyclicBarrier cb = new CyclicBarrier(currency); // 多线程模拟高并发 for (int i = 0; i &lt; currency; i++) &#123; new Thread(new Runnable() &#123; public void run() &#123; System.out.println(Thread.currentThread().getName() + \"---------我准备好---------------\"); // 等待一起出发 try &#123; cb.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; ZKDistributeImproveLock lock = new ZKDistributeImproveLock(\"/distLock111\"); try &#123; lock.lock(); System.out.println(Thread.currentThread().getName() + \" 获得锁！\"); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;).start(); &#125; &#125;&#125;","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"zk","slug":"zk","permalink":"https://huang-ding.github.io/hexo/tags/zk/"}]},{"title":"zookeeper核心概念","slug":"distributed-zk-core","date":"2020-05-07T11:55:40.000Z","updated":"2020-05-08T13:51:53.100Z","comments":true,"path":"2020/05/07/distributed-zk-core/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/07/distributed-zk-core/","excerpt":"","text":"Session会话 一个客户端链接一个会话，由zk分配唯一会话id；列:session id = 0x1001bb1f6720002 客户端以特定的时间间隔发送心跳以保持会话有效；tickTime 超过会话超时时间未收到客户端心跳，则判定客户端挂了；(默认2倍tickTime) minSessionTimeout : (No Java system property) New in 3.3.0: the minimum session timeout in milliseconds that the server will allow the client to negotiate. Defaults to 2 times the tickTime. maxSessionTimeout : (No Java system property) New in 3.3.0: the maximum session timeout in milliseconds that the server will allow the client to negotiate. Defaults to 20 times the tickTime. 会话中的请求按照FIFO(fast in fast on)顺序执行. 数据模型 层次名称空间 类型Unix文件系统，以(/)为根目录 区别：节点可以包含与之关联的数据以及子节点(即是文件也是文件夹) 节点的路径总是表示未规范的，绝对的，斜杠分隔的路径 znode 名称唯一，命名规范 null字符(\\u000)不能作为路劲的一部分 \\u0001-\\u0019和\\u007F-\\u009F不能使用，因为他们不能很好的显示，或者以令人困惑的方式 \\ud800-uf8fff,\\uFFF0-uFFFF不可使用 “.”字符可以用作另一个名称的一部分，但是“.”和”..”不能单独用于指示路劲上的节点，因为ZK不使用相对路径，“/a/b/./c”或“c/b/b/../”无效 “zookeeper”是保留节点名称 节点类型 持久 1create /app1 666 顺序 10位十进制序号 每个父节点一个计算器 计数器是带符号int(4字节)，到2147483647后将溢出为负值 12create -e /app2 666nodeName: app20000000001 临时 和客户端挂钩，当客户端断开时销毁 1create -s /app3 666 临时顺序 1create -s -e /app4 666 节点数据构成 节点数据：存储的协调数据(状态信息，配置信息，位置信息等) 节点元数据(stat结构) 数据量上限：1M 访问控制列表 ACLs are made up of pairs of (scheme:expression, perms)，the pair (ip:19.22.0.0/16, READ) ACL Permissions CREATE: you can create a child node READ: you can get data from a node and list its children. WRITE: you can set data for a node DELETE: you can delete a child node Builtin ACL Schemes world has a single id, anyone, that represents anyone. auth is a special scheme which ignores any provided expression and instead uses the current user, credentials, and scheme. Any expression (whether user like with SASL authentication or user:password like with DIGEST authentication) provided is ignored by the ZooKeeper server when persisting the ACL. However, the expression must still be provided in the ACL because the ACL must match the form scheme:expression:perms. This scheme is provided as a convenience as it is a common use-case for a user to create a znode and then restrict access to that znode to only that user. If there is no authenticated user, setting an ACL with the auth scheme will fail. digest uses a username:password string to generate MD5 hash which is then used as an ACL ID identity. Authentication is done by sending the username:password in clear text. When used in the ACL the expression will be the username:base64 encoded SHA1 password digest. ip uses the client host IP as an ACL ID identity. The ACL expression is of the form addr/bits where the most significant bits of addr are matched against the most significant bits of the client host IP. x509 uses the client X500 Principal as an ACL ID identity. The ACL expression is the exact X500 Principal name of a client. When using the secure port, clients are automatically authenticated and their auth info for the x509 scheme is set. Zookeeper中的时间 Zxid zookeeper中每次更改操作都对应一个唯一的事物id，称为zxid，它是一个全局有序的戳记，如果zxid1小于zxid2，则zxid1发送在zxid2之前 Version number 版本号，对节点的每次更改都会导该改节点的版本号之一增加 *Ticks * 用多服务器Zookeeper时，服务器使用‘’滴答‘’来定义时间的类型，如状态上传，会话超时，对等点之间的链接超时。滴答时间仅提供最小会话超时(滴答2倍)间接公开；如果客户端请求的会话时间小于最小会话超时，服务器将告诉客户端实际上是最小会话超时 RealTime Zookeeper除了在znode 创建和修改时将时间戳放入stat结构之外，其他根本不使用RealTime或时钟时间 watch机制 客户端可以在znodes上设置watch，监听znode变化 设置命令 -w 代表设置watch config [-c] [-w] [-s] get [-s] [-w] path ls [-s] [-w] [-R] path stat [-w] path watch类型 getData() 监听数据变化 getChildren() 监听子节点变化 exists() 是否存在 触发watch事件 Created event: Enabled with a call to exists. Deleted event: Enabled with a call to exists, getData, and getChildren. Changed event: Enabled with a call to exists and getData. Child event: Enabled with a call to getChildren. watch的重要特性 一次性触发：watch触发后即被删除，要持续监控变化，则需要持续的设置watch 有序性：客户端先得到watch通知，后才会看到变好结果 watch注意事项 watch是一次性触发器；如果您获得一个watch事件，并且希望得到关于未来的更改通知，需要设置另一个watch 因为watch是一次性触发器，并且在获取事件和发送获取watch的新请求之前存在时间间隔，所以不能可靠的获取到节点发生的每个更改 一个watch对象只会被特定的通知触发一次。如果一个watch对象同时注册了exists和getData，当节点删除事件对exists和getData都有效，但是只会调用一次 使用zkclient 监听watch事件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class ZkClientWatchDemo &#123; public static void main(String[] args) &#123; // 创建一个zk客户端 ZkClient client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); client.subscribeDataChanges(\"/mike/a\", new IZkDataListener() &#123; @Override public void handleDataDeleted(String dataPath) throws Exception &#123; System.out.println(\"----收到节点被删除了-------------\"); &#125; @Override public void handleDataChange(String dataPath, Object data) throws Exception &#123; System.out.println(\"----收到节点数据变化：\" + data + \"-------------\"); &#125; &#125;); try &#123; Thread.sleep(1000 * 60 * 2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;public class MyZkSerializer implements ZkSerializer &#123; String charset = \"UTF-8\"; @Override public Object deserialize(byte[] bytes) throws ZkMarshallingError &#123; try &#123; return new String(bytes, charset); &#125; catch (UnsupportedEncodingException e) &#123; throw new ZkMarshallingError(e); &#125; &#125; @Override public byte[] serialize(Object obj) throws ZkMarshallingError &#123; try &#123; return String.valueOf(obj).getBytes(charset); &#125; catch (UnsupportedEncodingException e) &#123; throw new ZkMarshallingError(e); &#125; &#125;&#125; Zookeeper特性 顺序一致性：保证客户端操作是按照顺序生效的 原子性：更新成功或失败，没有部分结果 单个系统映像：无论链接那个服务器，客户端都将看到相同的内容 可靠性：数据的变更不会丢失，除非被客户覆盖修改，数据变更时先写日志文件，再变更，如果是集群将发送集群变更命令，保证数据写入的可靠性 及时性：保证系统的客户端当时读取到的数据是最新的","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"zk","slug":"zk","permalink":"https://huang-ding.github.io/hexo/tags/zk/"}]},{"title":"zk入门","slug":"distributed-zk","date":"2020-05-06T11:57:23.000Z","updated":"2020-05-07T12:53:06.586Z","comments":true,"path":"2020/05/06/distributed-zk/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/06/distributed-zk/","excerpt":"","text":"什么是ZooKeeper起源：ZooKeeper 最早起源于雅虎研究院的一个研究小组。当时，雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。 简介：Apache Zookeeper是一种用于分布式应用程序的高性能协调服务，提供一种集中式信息存储服务 特点：数据存在内存中，类型文件系统的树形结构（文件和目录），高吞吐和低延迟，集群高可用 作用：基于zookeeper可以实现分布式统一配置中心，服务注册发现，分布式锁等 官网：https://zookeeper.apache.org/ 何为分布式协调服务 单机系统因处理能力上限，可用性，可靠性的考虑，变为分布式系统 原来的单机进程中完成的一件事的多个步骤，变为在多个计算机中完成，这时候就需要协调各个计算机节点做事情的顺序；原来在单系统中资源通过竞争通过锁进行同步控制；现在变成了多个计算机的进程之间的资源竞争，也需要资源协调。 我们可以把每个分布式系统中需要协调的协调管理的公共基础部分抽取出来作为一个基础公共服务，供大家使用，这就是分布式协调 zookeeper的应用案例 Hbase：使用Zookeeper进行Master选举，服务间协调 Solr：使用Zookeeper进行集群管理，Leader选举，配置中心 dubbo：服务注册发现 Mycat：集群管理，配置管理 Sharding-sphere：集群管理，配置管理 。。。。 zookeeper同类产品 consul，etcd，Doozer，等其中etcd和Doozer实现原理，存储方式和zk类似 单机版安装 官网下载对应版本压缩包 3.6.1版本安装包 解压后的conf目录，添加配置文件zoo.cfg（默认文件名，可以不使用此文件名，启动时指定便可） 启动服务端 bin/zkServer.sh start 测试，客户端链接：/bin/zkCli.sh -server 127.0.0.1:2181 123456789//单机版主要配置# The number of milliseconds of each tick 心跳时间tickTime=2000# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes. 数据存放目录dataDir=F:/zk/data# the port at which the clients will connect 客户端链接端口clientPort=2181 客户端操作命令 命令 命令参数 功能描述 addauth addauth scheme auth 创建 &amp;&amp; 验证用户 close - 关闭客户端 config config [-c] [-w] [-s] - connect connect host:port 客户端链接 create create [-s] [-e] [-c] [-t ttl] path [data] [acl] 创建节点 delete delete [-v version] path 删除节点 deleteall deleteall path 删除全部节点 rmr rmr path 递归删除节点 delquota delquota [-n |-b] path 用于删除已经创建的quota配额： get get [-s] [-w] path 获取节点值 getAcl getAcl [-s] path 获取节点信息 getAllChildrenNumber getAllChildrenNumber path 获取所有子节点数 getEphemeral getEphemerals path 获取临时节点 history - - listquota listquota path - ls ls [-s] [-w] [-R] path 获取节点信息 printwatches printwatches on off quit - 退出终端 reconfig [-s] [-v version] [[-file path] [-members serverID=host:port1:port2;port3[,…]*]] - redo redo cmdno - removewatches removewatches path [-c|-d|-a] [-1] - set set [-s] [-v version] path data 设置节点值 setAcl setAcl [-s] [-v version] [-R] path acl 设置节点权限 setquota setquota -n|-b val path 设置节点配额 stat stat [-w] path - sync sync path - version - - 第三方客户端-java12345678910111213141516171819202122&lt;!-- zkClient --&gt;&lt;!-- https://mvnrepository.com/artifact/com.101tec/zkclient --&gt;&lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;对应zk版本&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Curator --&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.curator/curator-recipes --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;对应zk版本&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt;&lt;/dependency&gt;","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"zk","slug":"zk","permalink":"https://huang-ding.github.io/hexo/tags/zk/"}]},{"title":"分布式系统架构演进之路","slug":"distributed-evolution","date":"2020-05-05T12:11:14.000Z","updated":"2020-05-05T14:45:50.583Z","comments":true,"path":"2020/05/05/distributed-evolution/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/05/05/distributed-evolution/","excerpt":"","text":"演进之路演进之路以一个网站为例 网站一开始就是大型的吗？ 我们应该一开始就设计一个大型网站吗？ 传统企业转型，钱多，召集上百人开发一个大型网站，等开发完结束上线，发现已经成千上百类似的网站，直接夭折。 出生 12graph LRA[无名小网站] --&gt;B[访问量低 一台服务器满足] 当业务发展越来越好，访问用户越来越多，面临问题 性能越来越差 越来越多的数据导致存储空间不足 解决方式 应用服务分离 数据服务分离 123456graph LRA[应用服务器,处理业务逻辑]B[数据库服务器,快速磁盘检索和数据缓存]C[文件服务器,存储用户上传文件]A--&gt;BA--&gt;C 不同的服务器承担不同的服务角色，并发处能利和数据存储空间性能提升 发展了一下 随着用户逐渐增加，网站再次面临挑战 数据库压力太大导致访问延迟，进行影响整个网站的性能，用户体验收到影响 当前用什么技术可以直接了当的解决问题，团队的使用了解情况，最简单的最快速的，技术的选型是有阶段性的，未到阶段不要使用太复杂的解决方案，给团队提高成本 解决方式，使用缓存改善性能 本地缓存 速度极快 数据量有限 和应用程序争抢内存 远程分布式缓存 按需扩展 性能相对本地较差 常用组件 redis memcache 系统架构图 随着用户逐渐增多，单一应用服务器面临新的问题： 能够处理的请求连接有限 网站访问高峰期 应用服务器成为网站瓶颈 解决方案 应用服务器集群，按需扩展 负载均衡调度服务器，需要高性能，高并发 软件 Apache Nginx Reverse-proxy pWEB LVS 硬件 F5 DNS负载均衡，利用域名对应多个IP 扩容（一般不考虑） 发展问题，使用缓存后虽然减轻了数据库压力，但是面临新的问题: 缓存访问不命中 缓存过期 全部的写操作都需要访问数据库 当达到一定规模后，数据库负载压力过大，成为系统瓶颈 解决方式 数据库读写分离 引发问题，数据库访问模块，数据存储层不应该和应用层代码有关系，解决方式 在Mybatis开发插件 mycat Sharding-JDBC 发展问题，用户规模导致地域越来越多，地域网络环境差别很大，面临问题 如何保证用户的访问体验 不至于访问慢流失用户 解决方案 反向代理 缓存静态资源 部署在数据中心，可以和负载均衡是同一个如nginx 地区CDN加速 适用于静态资源 部署在运营商，如电信运营商 加快用户访问速度，减轻后端服务器负载压力 发展问题，单文件服务器，单数据库，面临问题： 存不下日益增长的数据 解决问题： 分布式文件系统（存储小文件，图片） FastDFS TFS 分布式数据库系统(分库分表) Mycat Sharding-JDBC 发展问题，数据的存储需求和检索需求越来越复杂，面临问题 存储的字段差异较大，骷髅表 复杂的文本检索 解决方案： 使用NoSQL MongoDB elasticsearch 搜索引擎 lucene solr elasticsearch 发展问题，网站越来越好，业务越来越大，越来越复杂，面临问题： 应用程序变得无比庞大 迭代周期越来越快 牵一发动全身 怎么应对快速的业务发展需要 解决方案： 业务拆分，按照业务拆分多个为应用程序如，订单，首页，商家，推送 通过链接，MQ，数据存储建立连接 消息队列，RabbitMQ，ActiveMQ，Kafka 发展问题，业务规模不断增大，应用拆分越来越小，越来越多，面临问题 应用间的关系越来越复杂，应用中存在大量相同的业务操作 后端数据库成千上百台应用服务器连接，数据库连接池资源不足 解决方案 分布式服务(服务化) SOA架构 ESB 企业服务总线 中心点瓶颈 服务管理较好，可以在ESB处理 微服务 无中心点 服务框架 Dubbo SoringCloud 引用配置中心 Dubbo - zk SpringCloud config Disconf 百度 Config-toolkit 当当 Diamond 阿里 Apollo 携程 再往后，需要什么？ 数据挖掘 数据分析 推荐等业务需求 庞大的系统监控 问题分析 解决方案 大数据技术 Hadoop Spark 监控 Zabbix ElasticSearch+beats+Kibana 日志分析 ELK集中日志分析系统 以上为当前服务演进过程，当前的service mesh 还未开始琢磨，后续再修改 架构设计思想总结 分而治之 随着网站所需灵活应对 业务发展驱动技术发展，技术发展反哺业务 软件系统的价值在于它能够为用户提供什么价值，在于网站能做什么，而不在于它是怎么做的 不要新技术直接往上堆，考虑当前系统架构和团队技术栈 架构设计误区 一味追随大公司的解决方案 为了技术而技术 新技术可能存在多种系统漏洞 企图用技术解决所有问题 多探讨业务，看是否可以从业务层面解决问题 技术是用来解决业务问题的，而业务的问题，也可以通过业务的手段解决","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"设计思想","slug":"设计思想","permalink":"https://huang-ding.github.io/hexo/tags/%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/"}]},{"title":"java io 演进","slug":"evolution-io","date":"2020-04-29T13:18:53.000Z","updated":"2020-05-01T09:31:55.064Z","comments":true,"path":"2020/04/29/evolution-io/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/04/29/evolution-io/","excerpt":"","text":"JDK1.0—JDK1.3, java io类库非常原始，很多UNIX网络编程概念或接口在io库都没有体现，如：Buffer，Channel，Selector等 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//最传统单线程处理public class BIOService &#123; public static void main(String[] args) throws IOException &#123; //监听8080端口 ServerSocket socketServer = new ServerSocket(8080); System.out.println(\"启动服务端\"); while (!socketServer.isClosed()) &#123; //阻塞方法 Socket accept = socketServer.accept(); System.out.println(\"收到新链接\" + accept.toString()); try ( //接受数据 net +io InputStream inputStream = accept.getInputStream(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))) &#123; String msg; while ((msg = bufferedReader.readLine()) != null) &#123; if (msg.length() == 0) &#123; break; &#125; System.out.println(msg); &#125; System.out.println(\"收到数据：\" + accept.toString()); &#125; &#125; &#125;&#125;//客户端演示代码public class BIOClient &#123; public static void main(String[] args) &#123; try ( Socket s = new Socket(\"127.0.0.1\", 8080); OutputStream outputStream = s.getOutputStream(); Scanner scanner = new Scanner(System.in)) &#123; System.out.println(\"请输入：\"); String s1 = scanner.nextLine(); //阻塞 写完才返回 outputStream.write(s1.getBytes(StandardCharsets.UTF_8)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//使用多线程技术支持多链接 但是和线程池的大小有关 伪异步IOpublic class BIOService1 &#123; private static ExecutorService threadPool = Executors.newCachedThreadPool(); public static void main(String[] args) throws IOException &#123; //监听8080端口 ServerSocket socketServer = new ServerSocket(8080); System.out.println(\"启动服务端\"); while (!socketServer.isClosed()) &#123; //阻塞方法 Socket accept = socketServer.accept(); System.out.println(\"收到新链接\" + accept.toString()); //线程处理 threadPool.execute(() -&gt; &#123; try &#123; try ( //接受数据 net +io InputStream inputStream = accept.getInputStream(); BufferedReader bufferedReader = new BufferedReader( new InputStreamReader(inputStream, StandardCharsets.UTF_8)); OutputStream outputStream = accept.getOutputStream(); ) &#123; String msg; //读取数据阻塞 while ((msg = bufferedReader.readLine()) != null) &#123; if (msg.length() == 0) &#123; break; &#125; System.out.println(msg); &#125; System.out.println(\"收到数据：\" + accept.toString()); //返回http协议 //写入数据阻塞 outputStream.write(\"HTTP/1.1 200 ok \\r\\n\".getBytes()); outputStream.write(\"Content-Length: 11\\r\\n\\r\\n\".getBytes()); outputStream.write(\"Hello World\".getBytes()); outputStream.flush(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; &#125;&#125;//会引发的问题// 1.如果可用线程都出现故障阻塞，后续所有IO消息都将在队列排队// 2.当队列满后，后续入队操作将被阻塞// 3.新的客户端请求消息将被拒绝，客户端发送大量链接超时// 4.客户认为系统已经崩溃// NIO来解决 JDK1.4 NIO 以JSR-51 的身份正式发布，添加了java.nio包 进行异步IO的缓冲区ByteBuffer 缓存区容量(capacity) 缓冲区位置(position) 缓冲区限制(limit) 堆外缓存及堆内缓存 1234567891011//构建一个堆内内存4字节缓存区ByteBuffer byteBuffer = ByteBuffer.allocate(4);//构建一个堆外内存4字节缓存区,由于不由GC管理，使用是最好先指定JVM的最大堆外缓冲区大小限制；少一次堆拷贝ByteBuffer byteBuffer1 = ByteBuffer.allocateDirect(4);//缓冲区 相关api// flip() 读写反转// compact() 清除已读缓存区// clear() 清除整个缓冲区// rewind() 重置position为0// mark() 标记position的位置// reset() 重置position为上次mark()标记的位置 进行异步IO的管道Pipe 进行各种IO操作的Channel，包括ServerSocketChannel和SocketChannel Channel 通道，提供了NIO的非阻塞方法API，如文件FileChannel，SocketChannel，涵盖了UDP/TCP网络和文件IO，使用缓冲区进行IO操作 12345678910111213141516//SocketChannel和ServiceSocketChannel代码示例//SocketChannel//使用SocketChannel socketChannel = SocketChannel.open();//设置非阻塞模式,默认阻塞模式socketChannel.configureBlocking(false);//链接服务端socketChannel.connect(new InetSocketAddress(\"127.0.0.1\", 8080));//ServiceSocketChannel//创建网络服务端ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();//设置非阻塞serverSocketChannel.configureBlocking(false);//绑定端口serverSocketChannel.bind(new InetSocketAddress(8080)); 多种字符集的编码和解码 实现非阻塞IO操作的多路复用选择器Selector Selector选择器，可以通过Channel注册的方式进行检查管道的状态，从而实现单个线程管理多个Channel，从而提高NIO利用效率 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//Selector选择器实现public static void main(String[] args) throws Exception &#123; // 1. 创建网络服务端ServerSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); // 设置为非阻塞模式 // 2. 构建一个Selector选择器,并且将channel注册上去 Selector selector = Selector.open(); SelectionKey selectionKey = serverSocketChannel.register(selector, 0, serverSocketChannel);// 将serverSocketChannel注册到selector selectionKey.interestOps(SelectionKey.OP_ACCEPT); // 对serverSocketChannel上面的accept事件感兴趣(serverSocketChannel只能支持accept操作) // 3. 绑定端口 serverSocketChannel.socket().bind(new InetSocketAddress(8080)); System.out.println(\"启动成功\"); while (true) &#123; // 不再轮询通道,改用下面轮询事件的方式.select方法有阻塞效果,直到有事件通知才会有返回 selector.select(); // 获取事件 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); // 遍历查询结果e Iterator&lt;SelectionKey&gt; iter = selectionKeys.iterator(); while (iter.hasNext()) &#123; // 被封装的查询结果 SelectionKey key = iter.next(); iter.remove(); // 关注 Read 和 Accept两个事件 if (key.isAcceptable()) &#123; ServerSocketChannel server = (ServerSocketChannel) key.attachment(); // 将拿到的客户端连接通道,注册到selector上面 SocketChannel clientSocketChannel = server.accept(); // mainReactor 轮询accept clientSocketChannel.configureBlocking(false); clientSocketChannel.register(selector, SelectionKey.OP_READ, clientSocketChannel); System.out.println(\"收到新连接 : \" + clientSocketChannel.getRemoteAddress()); &#125; if (key.isReadable()) &#123; SocketChannel socketChannel = (SocketChannel) key.attachment(); try &#123; ByteBuffer requestBuffer = ByteBuffer.allocate(1024); while (socketChannel.isOpen() &amp;&amp; socketChannel.read(requestBuffer) != -1) &#123; // 长连接情况下,需要手动判断数据有没有读取结束 (此处做一个简单的判断: 超过0字节就认为请求结束了) if (requestBuffer.position() &gt; 0) break; &#125; if(requestBuffer.position() == 0) continue; // 如果没数据了, 则不继续后面的处理 requestBuffer.flip(); byte[] content = new byte[requestBuffer.limit()]; requestBuffer.get(content); System.out.println(new String(content)); System.out.println(\"收到数据,来自：\" + socketChannel.getRemoteAddress()); // TODO 业务操作 数据库 接口调用等等 // 响应结果 200 String response = \"HTTP/1.1 200 OK\\r\\n\" + \"Content-Length: 11\\r\\n\\r\\n\" + \"Hello World\"; ByteBuffer buffer = ByteBuffer.wrap(response.getBytes()); while (buffer.hasRemaining()) &#123; socketChannel.write(buffer); &#125; &#125; catch (IOException e) &#123; // e.printStackTrace(); key.cancel(); // 取消事件订阅 &#125; &#125; &#125; selector.selectNow(); &#125; // 问题: 此处一个selector监听所有事件,一个线程处理所有请求事件. 会成为瓶颈! 要有多线程的运用 // 使用Reactor线程模型处理问题&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226//单Reactor多线程模式public class ReactorService implements Runnable &#123; private final Selector selector; private final ServerSocketChannel serverSocketChannel; public static void main(String[] args) throws IOException &#123; ReactorService service1 = new ReactorService(8080); service1.run(); &#125; public ReactorService(int port) throws IOException &#123; //创建网络服务端 serverSocketChannel = ServerSocketChannel.open(); //设置非阻塞 serverSocketChannel.configureBlocking(false); //绑定端口 serverSocketChannel.bind(new InetSocketAddress(port)); //创建选择器 selector = Selector.open(); //注册网络服务端，监听链接创建事件 SelectionKey selectionKey = serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); //添加accept 处理事件 selectionKey.attach(new Acceptor()); System.out.println(\"启动完成\"); &#125; @Override public void run() &#123; try &#123; while (!Thread.interrupted()) &#123; //select方法有阻塞效果,直到有事件通知才会有返回 selector.select(); //获取事件 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); for (SelectionKey selectionKey : selectionKeys) &#123; //进行事件派发 dispatch(selectionKey); &#125; selectionKeys.clear(); selector.selectNow(); &#125; &#125; catch (IOException ignored) &#123; &#125; &#125; private void dispatch(SelectionKey selectionKey) &#123; //获取分发事件的附件 /Acceptor/Handler Runnable runnable = (Runnable) selectionKey.attachment(); if (null != runnable) &#123; runnable.run(); &#125; &#125; class Acceptor implements Runnable &#123; /** * 同步获取链接,保证链接获取在同一个Selector */ @Override public synchronized void run() &#123; try &#123; //获取新链接 SocketChannel socketChannel = serverSocketChannel.accept(); if (null != socketChannel) &#123; System.out.println(\"获取到新链接\"); //采用多路复用将事件分发给相应的Handler处理 new Handler(selector, socketChannel); &#125; &#125; catch (IOException i) &#123; &#125; &#125; &#125;&#125;final class Handler implements Runnable &#123; private final SocketChannel socketChannel; private final SelectionKey selectionKey; //处理业务的线程池 private static ExecutorService pool = Executors.newCachedThreadPool(); /** * 创建输入输出缓冲区 */ ByteBuffer input = ByteBuffer.allocate(1024); ByteBuffer output = ByteBuffer.allocate(1024); static final int READING = 0, SENDING = 1, PROCESSING = 3; int state = READING; public Handler(Selector selector, SocketChannel channel) throws IOException &#123; this.socketChannel = channel; //设置非阻塞 socketChannel.configureBlocking(false); //注册事件// SelectionKey.OP_ACCEPT selectionKey = socketChannel.register(selector, 0); //添加附件 selectionKey.attach(this); //表示感兴趣的事件通知 为写 selectionKey.interestOps(SelectionKey.OP_READ); //使Selector.select 返回下一次调用 selector.wakeup(); &#125; /** * 读处理 */ boolean inputIsComplete() throws IOException &#123; int read = socketChannel.read(input); if (read &gt; 0) &#123; input.flip(); byte[] content = new byte[input.limit()]; input.get(content); System.out.println(new String(content)); System.out.println(\"收到数据,来自：\" + socketChannel.getRemoteAddress()); &#125; else if (read &lt; 0) &#123; //对链路进行关闭 返回值-1 说明链路已关闭 selectionKey.cancel(); socketChannel.close(); return false; &#125; else &#123; //读取到0字节 忽略 &#125; return true; &#125; /** * 写处理 */ boolean outputIsComplete() throws IOException &#123; // 响应结果 200 String response = \"HTTP/1.1 200 OK\\r\\n\" + \"Content-Length: 11\\r\\n\\r\\n\" + \"Hello World\"; output.put(response.getBytes()); // hasRemaining 用于判断是否发送完成 防止'写半包'场景 while (output.hasRemaining()) &#123; socketChannel.write(output); &#125; output.clear(); System.out.println(\"写一点东西\"); return true; &#125; /** * 业务处理 */ void process() &#123; System.out.println(\"处理了一些业务\"); &#125; /** * 业务处理 */ void processHandOff() &#123; System.out.println(\"多线程业务处理\"); //修改当前状态 process(); state = SENDING; //表示感兴趣的事件通知 为读 selectionKey.interestOps(SelectionKey.OP_WRITE); &#125; @Override public void run() &#123; try &#123; if (state == READING) &#123; read(); &#125; else if (state == SENDING) &#123; send(); &#125; &#125; catch (IOException e) &#123; &#125; &#125; /** * 读处理 */ void read() throws IOException &#123; socketChannel.read(input); if (inputIsComplete()) &#123; //读完处理业务逻辑 线程池处理业务逻辑 state = PROCESSING; pool.execute(new Processer());// process();// //修改当前状态// state = SENDING;// //表示感兴趣的事件通知 为读// selectionKey.interestOps(SelectionKey.OP_WRITE); &#125; &#125; class Processer implements Runnable &#123; @Override public void run() &#123; processHandOff(); &#125; &#125; /** * 发送到请求方 */ void send() throws IOException &#123; if (outputIsComplete()) &#123; //写完，发送后，删除监听的事件 selectionKey.cancel(); &#125; &#125;&#125; 基于流行的Perl实现的正则表达式 文件通道FileChannel 。。。。。 新的NIO库极大的促进了java异步非阻塞编程的能力，但是还是有不完善的地方，特别是对文件的处理能力不足 没有统一的文件属性（如读写权限） API能力较弱，如目录的级联创建，递归遍历 底层存储系统的一些高级API无法使用 所有文件操作都是同步非阻塞，不支持异步文件读取 2011年JDK1.7发布。将原来的NIO类进行升级，被称为NIO2.0由JSR-203演进而来 提供可以批量获取文件属性的API，提供了标准文件系统SPI，供各个服务提供商扩展 提供AIO功能，支持基于文件的异步IO操作和针对网络套接字Socket的异步操作 完成JSR-51定义的通道功能，包括对配置和多播数据报的支持 提供异步套接字通道实现 通过juc.Future类实现异步操作的结果 在执行异步操作的时候传入CompletionHandler接口的实现类作为操作完成的回调 AIO实现demo 不需要通过多路复用选择器Selector对注册通道进行轮询操作即可实现异步读写，简化了NIO的编程模型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239//服务端public class AIOServer &#123; public static void main(String[] args) &#123; AsyncServerHandler asyncServerHandler = new AsyncServerHandler(8080); new Thread(asyncServerHandler, \"AIO-server\").start(); &#125;&#125;public class AsyncServerHandler implements Runnable &#123; private int port; CountDownLatch latch; AsynchronousServerSocketChannel asynchronousServerSocketChannel; public AsyncServerHandler(int port) &#123; try &#123; //构建异步通道 asynchronousServerSocketChannel = AsynchronousServerSocketChannel.open(); //绑定端口 asynchronousServerSocketChannel.bind(new InetSocketAddress(port)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void run() &#123; latch = new CountDownLatch(1); doAccept(); try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private void doAccept() &#123; //接受消息 asynchronousServerSocketChannel.accept(this, new AcceptCompleteHandler()); &#125;&#125;public class AcceptCompleteHandler implements CompletionHandler&lt;AsynchronousSocketChannel, AsyncServerHandler&gt; &#123; @Override public void completed(AsynchronousSocketChannel result, AsyncServerHandler attachment) &#123; //接受客户端链接，因为是异步所以继续调用accept方法，接受其他客户端链接。每当一个客户端链接成功后，再异步接受新的客户端 attachment.asynchronousServerSocketChannel.accept(attachment, this); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); //业务处理 result.read(byteBuffer, byteBuffer, new ReadCompleteHandler(result)); &#125; @Override public void failed(Throwable exc, AsyncServerHandler attachment) &#123; exc.printStackTrace(); attachment.latch.countDown(); &#125;&#125;public class ReadCompleteHandler implements CompletionHandler&lt;Integer, ByteBuffer&gt; &#123; private AsynchronousSocketChannel channel; public ReadCompleteHandler(AsynchronousSocketChannel channel) &#123; this.channel = channel; &#125; @Override public void completed(Integer result, ByteBuffer attachment) &#123; attachment.flip(); //获取数据大小 byte[] body = new byte[attachment.remaining()]; //获取数据 attachment.get(body); String req = new String(body, StandardCharsets.UTF_8); System.out.println(\"req：\" + req); //发送数据 doWrite(req); &#125; private void doWrite(String req) &#123; if (null != req &amp;&amp; req.length() &gt; 0) &#123; byte[] bytes = req.getBytes(); //创建缓冲区 ByteBuffer byteBuffer = ByteBuffer.allocate(bytes.length); byteBuffer.put(bytes); byteBuffer.flip(); //异步写入数据 channel.write(byteBuffer, byteBuffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; //未发送完成继续发送 if (attachment.hasRemaining()) &#123; channel.write(attachment, attachment, this); &#125; &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; //发生异常关闭连接 try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; //发生异常关闭连接 try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;//客户端public class AIOClient &#123; public static void main(String[] args) &#123; AsyncClientHandler asyncClientHandler = new AsyncClientHandler(\"127.0.0.1\",8080); new Thread(asyncClientHandler,\"AIOClient\").start(); &#125;&#125;public class AsyncClientHandler implements CompletionHandler&lt;Void, AsyncClientHandler&gt;, Runnable &#123; private final String host; private final int port; private AsynchronousSocketChannel channel; private CountDownLatch latch; public AsyncClientHandler(String host, int port) &#123; this.host = host; this.port = port; try &#123; channel = AsynchronousSocketChannel.open(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void run() &#123; latch = new CountDownLatch(1); //建立连接 channel.connect(new InetSocketAddress(host, port), this, this); try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; //释放连接 channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void completed(Void result, AsyncClientHandler attachment) &#123; byte[] req = \"Hello world\".getBytes(); ByteBuffer writer = ByteBuffer.allocate(req.length); writer.put(req); writer.flip(); channel.write(writer, writer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; //是否已经写完 if (attachment.hasRemaining()) &#123; //继续写 channel.write(attachment, attachment, this); &#125; else &#123; //写完获取服务端返回数据 ByteBuffer read = ByteBuffer.allocate(1024); channel.read(read, read, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; attachment.flip(); //读取数据 byte[] bytes = new byte[attachment.remaining()]; attachment.get(bytes); String body = new String(bytes, StandardCharsets.UTF_8); System.out.println(body); latch.countDown(); &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; latch.countDown(); &#125; &#125;); &#125; &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; latch.countDown(); &#125; &#125;); &#125; @Override public void failed(Throwable exc, AsyncClientHandler attachment) &#123; try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; latch.countDown(); &#125;&#125; 以上NIO演化 ，开始步入netty 开发高质量NIO程序不是一件简单的事情，除去NIO固有的复杂性和BUG，作为NIO服务端，需要处理网络的闪断，客户端重复接入，客户端安全认证，消息的编解码，半包读写等情况，如果没有足够的经验还是使用成熟NIO框架，netty是一个不错的选择","categories":[],"tags":[{"name":"nio","slug":"nio","permalink":"https://huang-ding.github.io/hexo/tags/nio/"}]},{"title":"I/O多路复用","slug":"multiplex-io","date":"2020-04-26T08:29:12.000Z","updated":"2020-04-29T13:14:24.820Z","comments":true,"path":"2020/04/26/multiplex-io/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/04/26/multiplex-io/","excerpt":"","text":"什么是I/O多路复用：通过把多个IO的阻塞复用到一个select的阻塞上，从而让系统在单线程的情况下可以处理多个客户端请求。 场景: 服务器需要同时处理多个处于监听状态或多个连接状态的套接字Socket 服务器需要同时处理多种网络协议的套接字。 目前支持I/O多路复用的系统调用有select，pselect，poll，epool，liunx在新的内核使用epoll，原因如下 支持一个进程打开的Socket描述符（FD）不受限制（仅受操作系统最大句柄数），select默认1024个，查看句柄数（cat /proc/sys/fs/file-max） I/O效率不会随着FD的条目增加而线性下降，epoll只针对‘活跃’的Socket进行操作，‘活跃’会主动调用callback函数 使用mmap加速内核与用户空间的消息传递，epoll通过内核和用户空间mmap同一块内存来实现的 epoll的API更简单","categories":[],"tags":[{"name":"nio","slug":"nio","permalink":"https://huang-ding.github.io/hexo/tags/nio/"}]},{"title":"I/O模型","slug":"nio","date":"2020-04-24T07:47:03.000Z","updated":"2020-04-29T13:14:19.930Z","comments":true,"path":"2020/04/24/nio/","link":"","permalink":"https://huang-ding.github.io/hexo/2020/04/24/nio/","excerpt":"","text":"liunx 网络I/O模型 阻塞I/O模型：一个线程处理一个IO，进程阻塞; 非阻塞I/O模型：缓冲区无数据，返回一个EWOULDBLOCK错误，一般进行轮询检查此状态; I/O复用模型：Liunx提供select/poll,进程通过将文件描述符(fd)传递给select/poll系统调用,阻塞在select操作上，这样select/poll可以侦测多个fd是否处于就绪状态；但是fd数量有限，受到一些制约，因此liunx还提供了一个epoll的系统调用，epool使用基于驱动方式代替顺序扫码，提高性能，当有fd就绪时，立即回调函数rollback; 信号驱动I/O模型：提高开启套接字信号驱动IO功能，并通过系统调用sigaction执行信号处理函数，当数据准备就绪时，就为改进程生成SIGIO信号，通过信号回调通知应用程序调用recvform函数读取数据，并通知主循环函数处理数据 异步I/O模型：告知内核启动某个操作，并让内核在整个操作完成后通知我们。","categories":[{"name":"unix网络模型","slug":"unix网络模型","permalink":"https://huang-ding.github.io/hexo/categories/unix%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"nio","slug":"nio","permalink":"https://huang-ding.github.io/hexo/tags/nio/"},{"name":"网络模型","slug":"网络模型","permalink":"https://huang-ding.github.io/hexo/tags/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}]}],"categories":[{"name":"unix网络模型","slug":"unix网络模型","permalink":"https://huang-ding.github.io/hexo/categories/unix%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"spring-cloud","slug":"spring-cloud","permalink":"https://huang-ding.github.io/hexo/tags/spring-cloud/"},{"name":"Docker","slug":"Docker","permalink":"https://huang-ding.github.io/hexo/tags/Docker/"},{"name":"dubbo","slug":"dubbo","permalink":"https://huang-ding.github.io/hexo/tags/dubbo/"},{"name":"SPI","slug":"SPI","permalink":"https://huang-ding.github.io/hexo/tags/SPI/"},{"name":"Spring","slug":"Spring","permalink":"https://huang-ding.github.io/hexo/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://huang-ding.github.io/hexo/tags/SpringBoot/"},{"name":"分布式","slug":"分布式","permalink":"https://huang-ding.github.io/hexo/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"rpc","slug":"rpc","permalink":"https://huang-ding.github.io/hexo/tags/rpc/"},{"name":"zk","slug":"zk","permalink":"https://huang-ding.github.io/hexo/tags/zk/"},{"name":"设计思想","slug":"设计思想","permalink":"https://huang-ding.github.io/hexo/tags/%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/"},{"name":"nio","slug":"nio","permalink":"https://huang-ding.github.io/hexo/tags/nio/"},{"name":"网络模型","slug":"网络模型","permalink":"https://huang-ding.github.io/hexo/tags/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}]}